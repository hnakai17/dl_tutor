{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will introduce both Google Colab and the Tensorflow 2 / Keras API, including demonstration of basic concepts related to statistical modeling and machine learning. An overview of topics covered in this tutorial include:\n",
    "\n",
    "**Google Colab**\n",
    "\n",
    "* Jupyter notebooks\n",
    "* mounting Google drive\n",
    "* environment setup\n",
    "\n",
    "**Tensorflow 2 / Keras API**\n",
    "\n",
    "* Tensorflow graphs\n",
    "* creating models\n",
    "* creating optimizers\n",
    "* creating loss functions\n",
    "* model fitting\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "### Jupyter\n",
    "\n",
    "A Jupyter notebook is composed of blocks of `Markdown` documentation or code referenced as cells. Each cell can be individually selected by a simple click. As you progress through this notebook, simply select a code-containing cell and click the `Run` button on the top toolbar (or alternatively `shift` + `[Enter]`) to execute that particular line or block of code. The `In [ ]` header to the left of each cell will change status to `In [*]` while a line or block of code is executing and then to a number indicating a line or block of executed code if successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and Keras\n",
    "\n",
    "Tensorflow is a free and open-source software library developed by the Google Brain team for dataflow and differentiable programming\n",
    "across a range of tasks. It is a symbolic math library, and is most popularly used for machine learning applications such as neural networks. In November 2019, the first stable release of the verson 2.0 library was made available, with significant changes including:\n",
    "\n",
    "* formal integration of the high-level Keras API for easy model building\n",
    "* `eager execution` of code, eliminating the need to manually compile man abstract syntax tree using a `session.run()` call\n",
    "* improved support for model deployment in production on any platform\n",
    "* improved support for distributed machine learning paradigms\n",
    "\n",
    "More information highlighting the key improvements can be found here: https://www.tensorflow.org/guide/effective_tf2\n",
    "\n",
    "## Import\n",
    "\n",
    "In this tutorial we will use the following Numpy and Tensorflow library components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning \n",
    "\n",
    "Machine learning models are **mapping functions** that learn to predict target output(s) based on provided input(s). Inputs may consist of a number of **features** derived from raw data (traditional machine learning models) or simply the **raw data** itself (neural networks). \n",
    "\n",
    "A machine learning model is defined by its **parameters**, numeric variables that are applied via **operations** on the algorithm inputs to yield desired outputs. Note that this defintion of learning is broad; in fact all conventional statistical models are systems with *learnable* parameters.  \n",
    "\n",
    "Consider a simple linear regression model:\n",
    "\n",
    "```\n",
    "y = m * x + b\n",
    "```\n",
    "\n",
    "As per above, this model meets all key specifications of a learnable system:\n",
    "\n",
    "* input: `x`\n",
    "* output: `y`\n",
    "* parameters: `m` and `b`\n",
    "\n",
    "Indeed, through repeated exposure to data samples, optimal values for `m` and `b` can be learned such that the target output `y` can be reliably predicted from any given input `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow graphs\n",
    "\n",
    "To implement a model with Tensorflow, one must reformulate an algorithm into a computational graph: a series of **operations** that define use of **parameters** to map provided input(s) to target output(s). Tensorflow / Keras is an object-oriented framework whereby the entire computational graph is created by passing `Tensor` objects into `layers` objects, yielding new (intermediate) `Tensor` objects which are fed into new `layers` objects, and so on. More explicitly, the overall process proceeds as follows:\n",
    "\n",
    "1. Define the model input(s) ==> returns instantiated `Tensor` object(s)\n",
    "2. Define first operation ==> returns instantiated `layers` object\n",
    "3. Pass the input `Tensor` object into the operation `layers` object ==> returns new (intermediate) Tensor object\n",
    "4. Define next operation ==> ...\n",
    "5. Pass intermediate `Tensor` object (from step 3) into new operation `layers` object ==> ...\n",
    "\n",
    "...and so on.\n",
    "\n",
    "In the above simple linear regression model, the multiplication (`m`) and addition (`b`) operators are combined into a single operation known as a **linear transformation**. In the context of neural networks, this operation is also synonmous with the term **densely-connected layer**. \n",
    "\n",
    "See below for an example of defining a simple computational graph to replicate a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an input\n",
    "x = Input(shape=(1,))\n",
    "print('x : type = {}'.format(type(x)))\n",
    "\n",
    "# --- Define a linear transform operation\n",
    "op = layers.Dense(1)\n",
    "print('op: type = {}'.format(type(op)))\n",
    "\n",
    "# --- Apply linear transform\n",
    "y = op(x)\n",
    "print('y : type = {}'.format(type(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layers\n",
    "\n",
    "In the context of neural networks, a **dense** layer refers to a simple matrix multiply operation (or dot product for vectors). Recall that matrices are defined by shape (`row`, `colummns`). Specifically:\n",
    "\n",
    "```\n",
    "Let:\n",
    "\n",
    "X = 1 x N matrix = [a, b, c, d, e...]\n",
    "\n",
    "M = N x 1 matrix = [[A],\n",
    "                    [B],\n",
    "                    [C],\n",
    "                    [D],\n",
    "                    [E],\n",
    "                    ...\n",
    "                    ]\n",
    "\n",
    "Then:\n",
    "\n",
    "X x M = ...\n",
    "\n",
    "= [[a, b, c, d, e...]] x [[A], = a * A + b * B + c * C + d * D + e * E ...\n",
    "                          [B],\n",
    "                          [C],\n",
    "                          [D],\n",
    "                          [E],\n",
    "                          ...\n",
    "                          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example with NumPy\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(1, 5)\n",
    "m = np.array([1, 2, 3, 4, 5]).reshape(5, 1)\n",
    "\n",
    "# --- Matrix multiply or \"dense\" operation\n",
    "np.matmul(x, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, a **dense** operation may be defined by a **bias** term, which is simply a constant that is added to the matrix multipy operation. In other words:\n",
    "\n",
    "```\n",
    "Let:\n",
    "\n",
    "X = 1 x N matrix (per above)\n",
    "M = N x 1 matrix (per above)\n",
    "b = constant\n",
    "\n",
    "Then: \n",
    "\n",
    "A \"dense\" operation = X x M + b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example with NumPy\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(1, 5)\n",
    "m = np.array([1, 2, 3, 4, 5]).reshape(5, 1)\n",
    "b = 5\n",
    "\n",
    "# --- Matrix multiply or \"dense\" operation with bias\n",
    "np.matmul(x, m) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this discussion, it is evident that a simple linear regression model can be in fact be defined using a **dense** layer with a single-element `m`, `x` and `y` terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "Let us first redefine the simple linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define lineaer regression model\n",
    "x = Input(shape=(1,))\n",
    "op = layers.Dense(1)\n",
    "y = op(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the shapes of the `x` and `y` Tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print tensor shapes\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the meaning of the `None` shape? This generic value indicates to Tensorflow that the first dimension of the `x` and `y` tensors can be **anything** (>= 1). In other words, if I have 10 observations then the shape of the `x` matrix will be `(10, 1)`, which when multiplied by a `(1, 1)` `m` matrix (+ the `b` bias term) will yield a `(10, 1)` output `y` matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model it is clear where `x` and `y` are defined, but where are the parameters `m` and `b`? The answer is that in Tensorflow, only input(s), output(s), intermediate `Tensors` are explicitly defined. By contrast parameters are maintained (and updated) implicitly by the Tensorflow library upon definition of operation `layers`. In other words, `m` and `b` in our model are created automatically by Tensorflow as soon as the operation (`op`) object is recieves its input `Tensor` object. Once instantiated both `m` and `b` are stored in the `op` object, and may be retrieved with the `get_weights(...`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- See parameters\n",
    "m, b = op.get_weights()\n",
    "print(m)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default in Tensorflow, multiplication parameters are set to random values (chosen via the `Glorot` intialization scheme) whereas addition parameters are set to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "\n",
    "Once the input(s), output(s) and all required operations have been defined, a Tensorflow `Model()` object can be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model by passing input(s) and output(s)\n",
    "model = Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass an arbitrary value (in the form of a NumPy array) into the model, use the `model.predict(...)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pass an input into the model\n",
    "model.predict(np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we replicate this operation using the `m` and `b` NumPy arrays retrieved from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Equivalent NumPy operation\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex models, it may be useful to visualize a summary of all intermediate operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print summary of model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a Model\n",
    "\n",
    "The current model parameters have been initialized to random values. Through exposure to data, the goal is for the model to *learn* optimal parameter values that allow for robust mapping of provided input to target output. To prepare the model for learning, a graph must be **compiled** through definition of, at minimum, the key following training components (each represented by Keras Python objects):\n",
    "\n",
    "* loss function\n",
    "* optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss object\n",
    "\n",
    "A loss function simply represents a formula that the machine can use to provide feedback regarding the quality of its current set of parameters. In other words, given a provided input `x` and a target output `y`, as well as model prediction `y'`, how does one quantify the *goodness* of the estimated output? Choosing a representative loss function is important as this feedback is used by the machine to improve its parameter values.\n",
    "\n",
    "In machine learning, any loss formulation can be used to estimate goodness of fit as long as the function is **differentiable**. Many pre-built loss functions encapsulated by Python classes are availabe for use in the `tf.losses.*` module.\n",
    "\n",
    "For a linear regression model, performance (e.g. fit) is most commonly evaluated by calculating the *squared distance* between the target output `y` and the model prediction `y'`. In other words, if a model predicts `5` when the target output is `2`, then the error is `(5 - 2) ** 2` or `9`. Thus, the parameters `m` and `b` that yield the **least squared error** for all data observations is defined to be optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a MSE loss\n",
    "loss = losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the loss object with some NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example MSE losses\n",
    "loss(y_true=np.array([1]), y_pred=np.array([5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an optimizer object\n",
    "\n",
    "An optimizer is a method used by the machine to improve its parameters. By definition, the parameters are updated such that the loss value (calculated by the loss function) decreases. A number of optimization methods have been described and are available through the `tf.optimizers.*` module. Currently, one of the most effective optimizers is the Adam technique which will be used in this tutorial (a good default choice for most tasks). \n",
    "\n",
    "In addition to optimizer technique, a learning rate specifying the *degree of change* per update step is required. For the purposes of this tutorial, we will use a default learning rate of `1e-3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "Once the model `optimizer` and `loss` objects have been defined, simply pass these objects into the `model.compile(...)` method to prepare for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now compiled and ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this tutorial, training data pairs (x, y) will be \"simulated\". Specifically (x, y) pairs will be drawn using the following equation:\n",
    "\n",
    "```\n",
    "y = m * x + b + random\n",
    "```\n",
    "\n",
    "Here `m` and `b` represent some ground-truth value for slope and intercept, while `random` represents some random number between `[0, 1]` to simulate noise in the data. For the purposes of demonstration, the values `m = 2` and `b = -1` will be used. \n",
    "\n",
    "The following lambda function can be used to create (x, y) training data pairs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define lambda function for random values [-0.5, +0.5]\n",
    "lo = -0.5\n",
    "hi = +0.5\n",
    "rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "# --- Define lambda function for linear transform\n",
    "m = 2\n",
    "b = -1\n",
    "f = lambda x : m * x + b + rand(x.shape)\n",
    "\n",
    "# --- Generate an example (x, y) pair\n",
    "xs = np.random.rand(1)\n",
    "ys = f(xs)\n",
    "print(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code can be used to visualize 100 random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate N number examples of data\n",
    "xs = np.random.rand(100)\n",
    "ys = f(xs)\n",
    "\n",
    "# --- Visualize\n",
    "import pylab\n",
    "pylab.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python generators\n",
    "\n",
    "There are a number of different ways to feed training data into a Tensorflow model. Recall above that a placeholder, the `Input()` object, was defined simply to identify the entrypoint(s) for data into the model. For algorithm training, real data matching the specifications of `Input()` above must be provided. The method of data input used throughout this class will be via **Python generators**.\n",
    "\n",
    "The Python generator pattern is a unique type of *function* that returns a **lazy generator**, a special iterable that does not store its contents in memory. Instead the contents *yielded* by the generator are created dynamically upon each loop. This is a very efficient way to iterate through datasets, especially as the total size of a dataset becomes large.\n",
    "\n",
    "The following lines of code wrap up the lambda function above in a Python generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(batch_size=128):\n",
    "    \"\"\"\n",
    "    Method to define a Python generator for training data\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Define lambda function for random values [-0.5, +0.5]\n",
    "    lo = -0.5\n",
    "    hi = +0.5\n",
    "    rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "    # --- Define lambda function for linear transform\n",
    "    m = 2\n",
    "    b = -1\n",
    "    f = lambda x : m * x + b + rand(x.shape)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        xs = np.random.rand(batch_size, 1) \n",
    "        ys = f(xs)\n",
    "        \n",
    "        yield xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Generator function recieves a single argument named `batch_size`; this variable represents the total number of training examples that will be generated during each loop. In addition, notice that the shape of both `x` and `y` are in fact `(batch_size, 1)`; this is defined to conform the `Input(...)` variable defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Recall model input shape\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code demonstrated basic Generator functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiating the generator object\n",
    "gen_train = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterating using for loop\n",
    "for xs, ys in gen_train:\n",
    "    print(xs.shape, ys.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterating using next(...)\n",
    "xs, ys = next(gen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing\n",
    "pylab.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Once the model has been compiled and the data prepared (via a generator), training can be invoked using the `model.fit(...)` method. The only parameters that need to be set relate to the number of total training iterations (total number of steps = `steps_per_epoch` x `epochs`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=500, \n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has converged, what do we expect that the algorithm learned value for `m` should be? How do we check this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "fname = './model.hdf5'\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model(fname, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The following exercise questions will assist in preparation for this lesson's assignment, to extend our single variable linear model to a multivariate linear regression:\n",
    "\n",
    "```\n",
    "y = m0 * x0 + m1 * x1 + m2 * x2 ... + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "What modifications to the model definition are needed to increase the number of inputs from 1 variable to 5 (e.g. 5-element vector), while keeping the prediction a single output (e.g. `y`)? Does the `shape` in input need to change? Does the argument in `Dense(...)` need to change? Use the following cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a new 5-element input model\n",
    "x = Input(shape=(?,))\n",
    "op = layers.Dense(?)\n",
    "y = op(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n",
    "\n",
    "Use `x.shape` and `y.shape` to check the input and output shapes. Keep in mind that`x.shape` should be `(None, 5`) for a 5-element input and `y.shape` should be `(None, 1)` for a single-element output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape should the weights in `op.get_weights(...)` represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get weights and check shape\n",
    "w = op.?\n",
    "print(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "What modifications to the lambda function are needed to extend the number of inputs from 1 variable to 5? Keep in mind that the following must be true:\n",
    "\n",
    "* the input `x` is of shape `(None, 5)`\n",
    "* the output `y` is of shape `(None, 1)` \n",
    "\n",
    "Use the following cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define new 5-element lambda function\n",
    "m = ?\n",
    "b = ?\n",
    "f = lambda x : ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n",
    "\n",
    "Start with an arbitrary `(None, 5)` input `x`, e.g. an array with shape `(2, 5)`. Given that `m` must also have 5 elements, what shape must `m` be in order to properly perform an `np.matmul(...)` operation? Does the order of multiplication matter e.g. `np.matmul(x, m)` vs. `np.matmul(m, x)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define arbitrary x and m\n",
    "x = np.random.rand(2, 5)\n",
    "m = np.array([0, 1, 2, 3, 4]).reshape(?)\n",
    "\n",
    "# --- Multiply together\n",
    "np.matmul(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a constant to your matrix multiply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multiply together + bias\n",
    "b = -1\n",
    "np.matmul(?, ?) + ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add random offsets to each of your output `y` values. What dimension of random numbers do you need to accomplish this? Note that if you add a **constant** (like the `b` bias term) you will simply be adding the same random value to **all** your `y` values, thus simply shifting your linear curve in the y-direction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define random number generator\n",
    "lo = -0.5\n",
    "hi = +0.5\n",
    "rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "    \n",
    "# --- Multiply together + bias + random\n",
    "np.matmul(?, ?) + ? + rand((?, ?))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
