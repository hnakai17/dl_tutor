{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Congratulations! You have reached the end of the CS 190 curriculum. As a final summative exercise, you will be tasked to develop a model to predict brain tumor patient survival using any of the approaches and tools you have learned this quarter. The brief tutorial will simply introduce the dataset and provide some strategies to help guide exploration. Once you are familiar with the task, you are welcome to move onto the assignment which contains more details regarding algorithm design requirements and submission.\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 3D volume will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast). The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/mr_brats_2020`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='mr/brats-2020-096')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "To specificy the correct Generator template file, pass a designated `keyword` string. In this exercise, we will be using brain MRI volumes that have been cropped to the boundaries of the tumor and resampled to a uniform 3D volume of shape (96, 96, 96, 4). Using this input, two separate target labels have been prepared:\n",
    "\n",
    "* survival scores (use `096*glb-org` keyword)\n",
    "* tumor segmentation labels (use `096*vox-org` keyword)\n",
    "\n",
    "To select the correct template and generators for this task, use the keyword string as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival scores\n",
    "\n",
    "To create generators yielding target survival score labels, use the `096*glb-org` keyword: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare survival score generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-096', keyword='096*glb-org')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))\n",
    "\n",
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('ys shape: {}'.format(ys['survival'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Survival scores**: Note that total days of patient survival have been converted to floating point scores between `[0, 1]` using the following formula:\n",
    "\n",
    "```\n",
    "score = log ( days ) / 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print score values\n",
    "print(ys['survival'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tumor segmentation\n",
    "\n",
    "To create generators yielding target tumor segmentation labels, use the `096*vox-org` keyword. In addition, use the following `configs` dictionary to ensure that the 4-class tumor segmentation labels are binarized. As an alternative to the `configs` dictionary you can use the same nested Python generator strategy that was used in the previous week 5 and 6 tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare tumor segmentation generators\n",
    "configs = {'specs': {'ys': {\n",
    "    'tumor': {\n",
    "        'dtype': 'uint8',\n",
    "        'loads': 'lbl-crp',\n",
    "        'norms': {'clip': {'min': 0, 'max': 1}},\n",
    "        'shape': [96, 96, 96, 1]}}}}\n",
    "\n",
    "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-096', keyword='096*vox-org', configs=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))\n",
    "\n",
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('ys shape: {}'.format(ys['tumor'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined\n",
    "\n",
    "To create a custom generator yielding *both* tumor segmentation and survival scores simultaneously, consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create custom configs dict\n",
    "configs = {'specs': {'ys': {\n",
    "    'tumor': {\n",
    "        'dtype': 'uint8',\n",
    "        'loads': 'lbl-crp',\n",
    "        'norms': {'clip': {'min': 0, 'max': 1}},\n",
    "        'shape': [96, 96, 96, 1]},\n",
    "    'survival': {\n",
    "        'dtype': 'float32',\n",
    "        'loads': 'survival_days_norm',\n",
    "        'shape': [1]}}}}\n",
    "\n",
    "# --- Create generators\n",
    "gen_train, gen_valid, client = datasets.prepare(\n",
    "    name='mr/brats-2020-096', \n",
    "    keyword='096*vox-org',\n",
    "    configs=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this code example, the `ys` dictionary is replaced by the custom `configs` specification above, so it does not matter which `keyword` you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The goal of this project is to perform **global survival prediction** for each patient (e.g., 3D volume of data). In other words, regardless of algorithm choice, the final objective is to predict a global survival score. This however does **not** mean that you are required to use global regression networks only; in fact it very well may be the case that a hybrid algorithm will overall perform better on this task.\n",
    "\n",
    "The task is designed to be open-ended on purpose. The only requirements are to:\n",
    "\n",
    "* test at minimum three different network architectures\n",
    "* one algorithm must use (at least) a global regression type loss function\n",
    "* one algorithm must use (at least) a pretrained autoencoder strategy\n",
    "\n",
    "While you can choose to be creative and employ any architecture that you like, the following discussion may help guide your development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global regression\n",
    "\n",
    "As a baseline, create a global regression network to predict survival from raw data. A global regression network is identical to a standard classifier except instead of a categorical prediction (trained using a cross-entropy loss) a continuous variable prediction is generated (trained using a regression loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a global regression network, consider a standard CNN backbone:\n",
    "\n",
    "```python\n",
    "# --- Define series of blocks\n",
    "l1 = conv1(16, inputs['dat'])\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "\n",
    "# --- Continue blocks as needed...\n",
    "```\n",
    "\n",
    "After a series of convolutional blocks, the final feature map needs to be transitioned to feature vectors using one of the following approaches:\n",
    "\n",
    "* serial convolutions (with stride > 1 or VALID type padding)\n",
    "* global pool operations (mean or max)\n",
    "* reshape / flatten operation\n",
    "\n",
    "For example, the flattening operation can be implemented as follows:\n",
    "\n",
    "```python\n",
    "f0 = layers.Flatten()(...)\n",
    "```\n",
    "\n",
    "See notes from week 3 for further information.\n",
    "\n",
    "After a number of (optional) hidden layer operations, the final logit score is generated as a single-element prediction normalized using a *sigmoid* function (e.g., all survival scores are between `[0, 1]`):\n",
    "\n",
    "```python\n",
    "logits['survival'] = layers.Dense(1, activation='sigmoid', name='survival')(...)\n",
    "```\n",
    "\n",
    "After creating the `model` object, ensure that the model is trained using a regression loss such as *mean absolute error* or *mean squared error*:\n",
    "\n",
    "```python\n",
    "model.compile(..., loss={'survival': losses.MeanSquaredError()}, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "The autoencoder architecture is designed to create a compressed latent feature representation of the original input data. By compressing the raw data using an autoencoder, only the most relevant, critical details of the original input are retained in the low dimensional feature representation. See notes from week 9 for overview of creating a pretrained autoencoder network.\n",
    "\n",
    "For your final (third) model, if you choose to modify and/or extend the autoencoder approach, consider any of the following strategies.\n",
    "\n",
    "#### Modifications to the autoencoder network\n",
    "\n",
    "The baseline autoencoder network may be modified using:\n",
    "\n",
    "* increased feature map depth (channel size)\n",
    "* increased total number layers\n",
    "* added complexity through motifs (e.g., ResNet, Inception, DenseNet, etc)\n",
    "* improved optimization (e.g., more training iterations, learning rate decay, etc)\n",
    "* alternate loss functions (e.g., MAE)\n",
    "\n",
    "Recall that given the autoencoder design (e.g., the \"compressed\" latent representation), it is relatively unlikely to overfit the autoencoder network itself so model complexity may be increased quite a bit using many of these modifications simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications to the autoencoder task\n",
    "\n",
    "The standard autoencoder is trained to complete a reconstruction task, however any task may be used to guide feature learning. In this dataset, the tumor segmentation task may provide an alternate (possibly more meaningful) feature representation (e.g., the assumption is that features used to perform tumor segmentation are *also* useful for predicting patient survival).\n",
    "\n",
    "To do so, first create a standard U-Net derived model:\n",
    "\n",
    "```python\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, inputs['dat'])\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "\n",
    "# --- Define expanding layers\n",
    "l6  = tran2(48, l5)\n",
    "l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
    "l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
    "l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
    "l10 = conv1(8,  l9)\n",
    "```\n",
    "\n",
    "From these defined layers, we create both the standard U-Net and a separate encoder just as before:\n",
    "\n",
    "```python\n",
    "# --- Create U-net\n",
    "logits = {'tumor': layers.Conv3D(filters=2, name='tumor', **kwargs)(l10)}\n",
    "unet = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Create encoder\n",
    "encoder = Model(inputs=inputs, outputs=l5)\n",
    "```\n",
    "\n",
    "This `unet` model may be compiled and trained using standard strategies. See week 5 and 6 notes for further information. Be sure to use the correct generator to train this model with segmentation mask outputs (e.g., use keyword `096*vox-org`). Afterwards, use the `encoder` to build a fine-tuned prediction network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications to the fine-tuned prediction network\n",
    "\n",
    "Regardless of pretrained model of choice (e.g., standard autoencoder or U-Net) the final goal is create a fine-tuned model for survival score prediction. See week 9 notes for further information.\n",
    "\n",
    "The baseline fine-tuned prediction network may be modified using:\n",
    "\n",
    "* modified method for transition from feature maps to feature vectors (e.g., pooling)\n",
    "* increased *or* decreased size or number of hidden layer nodes\n",
    "* use of reguarlization (e.g., dropout, L2 normalization)\n",
    "* alternate loss function\n",
    "\n",
    "Note that while the pretrained model (e.g., autoencoder or tumor segmentation) is unlikely to overfit significantly, this fine-tuned prediction network will very easily overfit with the relatively small number of patients in this cohort. Thus, the strategies chosen above should be carefully tuned to optimize model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Loss\n",
    "\n",
    "Instead of pretraining a model and fine-tuning as separate (asynchronous) training sessions, it is possible and potentially beneficial to optimize both loss functions simultaneously. \n",
    "\n",
    "Consider the following autoencoder and survival prediction model with two separate outputs:\n",
    "\n",
    "```python\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, inputs['dat'])\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "\n",
    "# --- Define expanding layers\n",
    "l6  = tran2(48, l5)\n",
    "l7  = tran2(32, conv1(48, l6))\n",
    "l8  = tran2(16, conv1(32, l7))\n",
    "l9  = tran2(8,  conv1(16, l8))\n",
    "l10 = conv1(8,  l9)\n",
    "\n",
    "# --- Define survival prediction\n",
    "h0 = layers.Flatten()(l5)\n",
    "h1 = layers.Dense(32, activation='relu')(h0)\n",
    "\n",
    "# --- Define all logits\n",
    "logits = {}\n",
    "logits['survival'] = layers.Dense(1, activation='sigmoid', name='survival')(h1)\n",
    "logits['recon'] = layers.Conv3D(filters=4, name='recon', **kwargs)(l10)\n",
    "```\n",
    "\n",
    "After creating our model, be sure to compile the model with two separate loss functions:\n",
    "\n",
    "```python\n",
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss={\n",
    "        'survival': losses.MeanSquaredError(),\n",
    "        'recon': losses.MeanSquaredError()},\n",
    "    experimental_run_tf_function=False)\n",
    "```\n",
    "\n",
    "In this example, both the reconstruction and the survival scores are trained with a `mean squared error` loss function, but as you can see, any loss function can be used. Additionally, any number of training `metrics` can be used using a similar approach by specifying metrics as Python dictionaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-convolutional network\n",
    "\n",
    "Both the autoencoder and tumor segmentation models (e.g., fully convolutional contracting-expanding networks) are highly *regularized* through the prediction of a dense matrix of outputs (e.g., 1-to-1 mapping in number of input and output matrix shapes). Is there a method to adopt this approach to a regression task?\n",
    "\n",
    "The simplest approach is to reuse the standard U-Net network and simply replace the categorical loss (e.g., cross-entropy) with a regression loss.\n",
    "\n",
    "```python\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, inputs['dat'])\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "\n",
    "# --- Define expanding layers\n",
    "l6  = tran2(48, l5)\n",
    "l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
    "l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
    "l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
    "l10 = conv1(8,  l9)\n",
    "\n",
    "# --- Create model\n",
    "logits = {'survival': layers.Conv3D(filters=1, name='survival', **kwargs)(l10)}\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
    "    loss={'survival': losses.MeanSquaredError()},\n",
    "    experimental_run_tf_function=False)\n",
    "```\n",
    "\n",
    "To train this network, you will need a new custom Python generator that yields a single element `ys` output that is a dense `(96, 96, 96, 1)` tensor with identical survival score values. Assuming you are starting with the standard generator `survival` generator above, then consider the following code:\n",
    "\n",
    "```python\n",
    "def generator(G):\n",
    "    \"\"\"\n",
    "    Method to modify combined generator \n",
    "    \n",
    "    \"\"\"\n",
    "    for xs, ys in G:\n",
    "\n",
    "        # --- Reshape survival to 5D tensor\n",
    "        survival = ys['survival'].reshape(-1, 1, 1, 1, 1)\n",
    "        \n",
    "        # --- Replace ys['survival']\n",
    "        ys['survival'] = np.zeros((survival.shape[0], 96, 96, 96, 1), dtype='float32')\n",
    "        ys['survival'][:] = survival\n",
    "\n",
    "        yield xs, ys\n",
    "```\n",
    "\n",
    "Once your algorithm is trained, each single patient 3D volume will result in a 3D matrix of survival score predictions. To generate a single global prediction, consider taking the mean, median or other statistical aggregation method across all pixel values.\n",
    "\n",
    "Finally, note that in combining several strategies from this entire document, one may create a fully-convolutional model with *multiple* loss functions that can be trained simultaneously. For example, a simple modification to the above can be used to create a model that predictions both survival and tumor segmentations:\n",
    "\n",
    "```python\n",
    "# --- Create model\n",
    "logits = {}\n",
    "logits['survival'] = layers.Conv3D(filters=1, name='survival', activation='sigmoid', **kwargs)(l10)\n",
    "logits['tumor'] = layers.Conv3D(filters=2, name='tumor', **kwargs)(l10)\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
    "    loss={\n",
    "        'survival': losses.MeanSquaredError(),\n",
    "        'tumor': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "    experimental_run_tf_function=False)\n",
    "```\n",
    "\n",
    "What is the appropriate custom generator code that will yield the correct two target outputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
