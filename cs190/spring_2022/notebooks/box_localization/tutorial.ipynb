{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore the foundations of box localization networks. First we will examine the most common anchor parameterization of boxes at various scales and ratios across different feature map levels. Second we will explore the a popular backbone common to many modern box localization networks: the feature pyramid network. Finally we will dive into specifies regarding a popular high-performing implementation: RetinaNet.\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, models, layers, optimizers\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow\n",
    "from jarvis.train.box import BoundingBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about he KiTS Challenge can be found here: https://kits21.kits-challenge.org/. In this exercise, we will use this dataset to derive a model for slice-by-slice kidney segmentation. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_kits`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='ct/kits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality. As needed, pass any custom configurations (e.g. batch size, normalization parameters, etc) into the optional `configs` dictionary argument. \n",
    "\n",
    "To specificy the correct Generator template file, pass a designated `keyword` string. In this tutorial, we will be using abdominal CT volumes that have been preprocessed into 96 x 96 x 96 matrix volumes, each cropped to the right and left kidney, facilitating ease of algorithm training within the Google Colab platform. To select the correct Client template for this task, use the keyword string `2d-bin`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='2d-bin', custom_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield a total of `batch['size']` training samples based on the specified batch size. As before, each iteration yields dictionary of model inputs, `xs`. In the current example, there is just a single input image `xs['dat']` and a single target `xs['lbl']`. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, _ = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "for k, v in xs.items():\n",
    "    print('key = {} : shape = {}'.format(k.ljust(7), v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD5ejoTbx1_0"
   },
   "source": [
    "### KITS Data\n",
    "\n",
    "The input images in the variable `dat` are matrices of shape `1 x 96 x 96 x 1`. Note that even though the images here are 2D in shape, the full matrix is a 3D tensor `(z, y, x)` where `z = 1` in this implementation. Note that although the 3rd z-axis dimension is redundant here (for a single slice input), many of our more complex models and architectures will commonly require a full 3D tensor. Because of this, we will directly use 3D convolutions throughout the tutorial materials for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example\n",
    "imshow(xs['dat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images\n",
    "imshow(xs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney masks\n",
    "\n",
    "The ground-truth labels are binary masks of the same matrix shape as the model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs['lbl'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `imshow(...)` method to visualize the ground-truth tumor mask labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show tumor masks overlaid on original data\n",
    "imshow(xs['dat'], xs['lbl'])\n",
    "\n",
    "# --- Show tumor masks isolated\n",
    "imshow(xs['lbl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Parameterization\n",
    "\n",
    "Recall the common parameterization of boxes across an image using a grid of anchors:\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/box_params.png)\n",
    "\n",
    "At each anchor location, a total of **A** anchors may be defined spanning a variety of:\n",
    "\n",
    "* **aspect ratios**: 1:1, 2:1, 1:2, etc ...\n",
    "* **scales**: 2 ** 0, 2 ** (1/3), 2 ** (2/3), etc ...\n",
    "\n",
    "For each **A** number of anchors, there are two separate predictions:\n",
    "\n",
    "* **K**-element logit score representing a binary prediction of whether or not the *k-th* class is present in the box\n",
    "* **4**-element box fine-tuning representing the shift in the height, width, y- and x-coordinates from base box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor grid sizes\n",
    "\n",
    "The H x W size of an anchor grid (e.g. and thus implicitly the correspond box size) is commonly referenced by the **number of subsamples** required relative to the original full image shape. For example, if an original image is (N, N) in shape, then the first subsampled feature map is (N / 2, N / 2), the second subsampled featured map is (N / 4, N / 4), etc...\n",
    "\n",
    "In this example, the original input images are (96, 96) MR images. Thus the following feature maps (prefixed with `c`) may be defined:\n",
    "\n",
    "* **c1**: 48 x 48 anchor grid\n",
    "* **c2**: 24 x 24 anchor grid\n",
    "* **c3**: 12 x 12 anchor grid\n",
    "* **c4**: 6 x 6 anchor grid\n",
    "\n",
    "... and so on. By default, it is most common to start at the `c2` or `c3` level and proceed to include 2 to 5 different resolutions depending on the desired target.\n",
    "\n",
    "### `BoundingBox`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BoundingBox` class as part of the `jarvis-md` library facilitates definition and manipulation of boxes parameterized using the above standard notation. The object initializer has the following arguments:\n",
    "\n",
    "```\n",
    "(iter)   image_shape     : original 2D image shape\n",
    "(int)    classes         : number of non-background classes\n",
    "(iter)   c               : feature maps to use; c1 = 1st subsample, c2 = 2nd subsample, etc\n",
    "(iter)   anchor_shapes   : base shape of anchors in each feature map\n",
    "(iter)   anchor_scales   : scales of each anchor parameterized as 2 ** (i/3)\n",
    "(iter)   anchor_ratios   : aspect ratios of each anchor\n",
    "(float)  iou_upper       : upper IoU used for pos boxes\n",
    "(float)  iou_lower       : lower IoU used for neg boxes\n",
    "(float)  iou_nms         : IoU used for non-max supression\n",
    "(int)    box_padding     : padding for ground-truth boxes\n",
    "(bool)   separate_maps   : if True, create parameters for each feature map separately\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create BoundingBox\n",
    "bb = BoundingBox(\n",
    "    image_shape=(96, 96),\n",
    "    classes=1,\n",
    "    c=[3, 4],\n",
    "    anchor_shapes=[16, 32],\n",
    "    anchor_scales=[0, 1, 2],\n",
    "    anchor_ratios=[0.5, 1, 2],\n",
    "    iou_upper=0.5,\n",
    "    iou_lower=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will initialze all anchors and template boxes based on our specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# PRINT BOUNDING BOX SPECS\n",
    "# =============================\n",
    "\n",
    "# --- Print grid sizes\n",
    "print('---------------------------------------')\n",
    "print('Anchor Grid Sizes')\n",
    "print(bb.params['anchor_gsizes'])\n",
    "\n",
    "# --- Print template anchor box shapes\n",
    "print('---------------------------------------')\n",
    "print('Anchor Template Box Shapes')\n",
    "print(bb.params['anchor_shapes'])\n",
    "\n",
    "# --- Print anchor details\n",
    "print('---------------------------------------')\n",
    "print('Anchor scales: {}'.format(bb.params['anchor_scales']))\n",
    "print('Anchor ratios: {}'.format(bb.params['anchor_ratios']))\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('Total anchors (A) = {}'.format(\n",
    "    len(bb.params['anchor_scales']) * \n",
    "    len(bb.params['anchor_ratios'])))\n",
    "print('Total classes (k) = {}'.format(bb.params['classes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other parameters can be found in `bb.params`.\n",
    "\n",
    "**Checkpoint**: How many boxes in total are defined by the specifications above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground-truth\n",
    "\n",
    "Recall that the ground-truth predictions the box-localization CNN must produce are variable depending on the box parameterization chosen above. Predictions at *multiple feature map resolutions* must be provided for both the classification task (e.g. determine which boxes are positive) and regression task (e.g. determine what modifications are needed to template boxes to create final boxes). \n",
    "\n",
    "**Checkpoint**: How many different feature map predictions must the CNN generate in the box parameterization chosen above? What are the shapes for all predicted feature maps? Use `bb.params['inputs_shapes']` to confirm your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BoundingBox` object can create ground-truth box parameterizations using either label masks (e.g. provided in this tutorial) or boxes provided in anchor-style format e.g `[y0, x0, y1, x1]`. To generate ground-truth from a provided mask, use the `bb.convert_msk_to_box(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create box ground-truths\n",
    "box = bb.convert_msk_to_box(xs['lbl'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the parameterized box ground-truths match your expected tensor shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw box parameterizations are difficult to visualize and/or check. Instead to *post-process* box parameterizations, use one of the following methods:\n",
    "\n",
    "* `bb.convert_box_to_anc(...)`: method to convert box to anchors (e.g. `[y0, x0, y1, x1]`)\n",
    "* `bb.convert_box_to_msk(...)`: method to convert box to mask label for visualization\n",
    "\n",
    "For both methods, the `apply_deltas=[True/False]` flag can be used to specify whether or not to apply the box refinements (e.g. regression network predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to anchors\n",
    "anchors, classes = bb.convert_box_to_anc(box, apply_deltas=False)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('\\nGround-truth template boxes (before refinement):\\n')\n",
    "print(anchors)\n",
    "\n",
    "anchors, classes = bb.convert_box_to_anc(box, apply_deltas=True)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('\\nGround-truth template boxes (after refinement):\\n')\n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: why are the post-refinement boxes exactly identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to mask (for visualization)\n",
    "msk = bb.convert_box_to_msk(box, apply_deltas=False)\n",
    "imshow(xs['dat'][0, ..., 0], msk, title='Ground-truth template boxes (before refinement)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to mask (for visualization)\n",
    "msk = bb.convert_box_to_msk(box, apply_deltas=True)\n",
    "imshow(xs['dat'][0, ..., 0], msk, title='Ground-truth template boxes (after refinement)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: What happens to the appearance of boxes with variations in:\n",
    "\n",
    "* grid sizes (`c` values)\n",
    "* anchor shapes\n",
    "* anchor aspect ratios\n",
    "* anchor scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "\n",
    "To convert the current mask (segmentation output) generators into bounding box labels, use a nested Python generators using the `bb.convert_msk_to_box(...)` method as demonstrated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_generator(G):\n",
    "    \n",
    "    for xs, _ in G:\n",
    "        \n",
    "        # --- Convert mask into bounding-box paramaterization\n",
    "        msk = xs.pop('lbl')\n",
    "        box = bb.convert_msk_to_box(msk=msk)\n",
    "        \n",
    "        # --- Update xs dictionary\n",
    "        xs.update(box)\n",
    "        \n",
    "        yield xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to initialize training and validation generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid = client.create_generators()\n",
    "gen_train = box_generator(G=gen_train)\n",
    "gen_valid = box_generator(G=gen_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize within the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show first iteration\n",
    "xs = next(gen_train)\n",
    "msk = bb.convert_box_to_msk(box=xs, apply_deltas=False)\n",
    "imshow(xs['dat'][:, 0], msk[:, 0], figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pyramid Network\n",
    "\n",
    "Now that the inputs and outputs of the CNN have been defined, the goal is to implement a network architecture that is able to perform the desired mapping via a feature pyramid network. The contracting arm of a FPN architecture is nonspecific and can be implemented using any standard architecture.\n",
    "\n",
    "Let us define the contracting arm as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contracting arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "\n",
    "# --- Input input\n",
    "x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
    "\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, x)\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(24, conv2(24, l2))\n",
    "l4 = conv1(32, conv2(32, l3))\n",
    "l5 = conv1(48, conv2(48, l4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: The most important part here is simply to ensure that the deepest layer is at least the same size (or smaller) than the smallest anchor grid size needed for the box network. How can we confirm this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding arm\n",
    "\n",
    "To create layers of the expanding arm of the FPN, two special new operations must be defined.\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/fpn.png)\n",
    "\n",
    "First, to upsample an FPN feature map, a simple parameterless interpolation is used. The corresponding Tensorflow class is the `layers.UpSampling3D(...)` object. Let us define the corresponding lambda function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define zoom\n",
    "zoom = lambda x : layers.UpSampling3D(\n",
    "    size=(1, 2, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in order to add the corresponding contract arm layer, recall that a 1 x 1 x 1 convolution must be used to **match feature map channels (filters)**. Recall that all FPN output maps must have the same identical number of channels (in our case, 64). Let us define the corresponding lambda function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define 1 x 1 x 1 projection\n",
    "proj = lambda filters, x : layers.Conv3D(\n",
    "    filters=filters,\n",
    "    strides=1,\n",
    "    kernel_size=(1, 1, 1),\n",
    "    padding='same',\n",
    "    kernel_initializer='he_normal')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the expanding layers. Recall that we only need to create the required anchor grid sizes as defined above:\n",
    "\n",
    "* c4: 6 x 6\n",
    "* c3: 12 x 12\n",
    "\n",
    "Once these have been created, there is no need to define more expansions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define expanding layers\n",
    "l6 = proj(64, l5)\n",
    "l7 = zoom(l6) + proj(64, l4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize the `c3` and `c4` feature maps, simply apply one final 3 x 3 convolution to each intermediate tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finalize feature maps\n",
    "c4 = conv1(64, l6)\n",
    "c3 = conv1(64, l7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet\n",
    "\n",
    "Now that the FPN backbone has been created, we must finalize the feature maps to perform the classification and regression tasks necessary to predict boxes. There are many ways to implement this final mapping; we will derive a method based off of the approach described in the RetinaNet paper. This implementation is quite simple (most of the \"power\" lies in the focal loss function) and simply requires that at each feature map resolution, a classifier head is created to perform the necessary classification and regression tasks.\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/retinanet.png)\n",
    "\n",
    "In the original RetinaNet paper, four convolutional blocks are used, however in the context of medical imaging problems (e.g. less data, more homogenous predictions), we will just use two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Determine filter sizes\n",
    "logits = {}\n",
    "K = 1\n",
    "A = 9\n",
    "\n",
    "# --- C3\n",
    "c3_cls = conv1(64, conv1(64, c3))\n",
    "c3_reg = conv1(64, conv1(64, c3))\n",
    "logits['cls-c3'] = layers.Conv3D(filters=(A * K), name='cls-c3', **kwargs)(c3_cls)\n",
    "logits['reg-c3'] = layers.Conv3D(filters=(A * 4), name='reg-c3', **kwargs)(c3_reg)\n",
    "\n",
    "# --- C4\n",
    "c4_cls = conv1(64, conv1(64, c4))\n",
    "c4_reg = conv1(64, conv1(64, c4))\n",
    "logits['cls-c4'] = layers.Conv3D(filters=(A * K), name='cls-c4', **kwargs)(c4_cls)\n",
    "logits['reg-c4'] = layers.Conv3D(filters=(A * 4), name='reg-c4', **kwargs)(c4_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the `backbone` can be formally created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "backbone = Model(inputs=x, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `backbone` model architecture is wrapped in a second model with additional layer(s) that define optimization behavior including loss function derivations.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "As before, we start by defining all `inputs` into our new *wrapper* model. For box localization networks recall that the loss function is defined across multiple resolutions (`c3`, `c4`, etc) and for both classification and regression tasks. Thus, the `xs` input has multiple entries and the corresponding `inputs` dict here much be adjusted accordingly. The use of these additional model inputs will be discussed further in the following sections later in this tutorial. For now, let us first check the shape of the `xs` entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print shapes of all model inputs\n",
    "{k: v.shape for k, v in xs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following Python dict comprehension to create corresponding `Input` objects for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: Input(shape=[None] + list(v.shape[2:]), dtype='float32', name=k) for k, v in xs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `inputs` Python dictionary, let us first recreate the CNN model operations by **reusing** the `backbone` object that we already defined. Doing so means that our new *wrapper* model is explicitly derived from the `backbone`. Any updates applied to our new *wrapper* model are propogated to the `backbone` model and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define first step of new wrapper model\n",
    "logits = backbone(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss\n",
    "\n",
    "To account for inherent class imbalance, box localization models may be implemented with **focal loss**. As you recall, the **focal loss** function gradually titrates the contribution of any given prediction such that more confident correct predictions over time become weighted less than incorrect predictions.\n",
    "\n",
    "![Focal Loss](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2022/notebooks/box_localization/pngs/focal_loss.png)\n",
    "\n",
    "Focal loss is not a default loss function built into the standard Tensorflow 2 / Keras library. Accordingly, use the following custom implementation of focal loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_sigmoid_ce(y_true, y_pred, weights=1.0, scale=1.0, gamma=2.0, alpha=0.25, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement focal sigmoid (binary) cross-entropy loss\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Calculate standard cross entropy with alpha weighting\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        labels=y_true, logits=y_pred, pos_weight=alpha)\n",
    "\n",
    "    # --- Calculate modulation to pos and neg labels \n",
    "    p = tf.math.sigmoid(y_pred)\n",
    "    modulation_pos = (1 - p) ** gamma\n",
    "    modulation_neg = p ** gamma\n",
    "\n",
    "    mask = tf.cast(y_true, dtype=tf.bool)\n",
    "    modulation = tf.where(mask, modulation_pos, modulation_neg)\n",
    "\n",
    "    return tf.math.reduce_mean(modulation * loss * weights * scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber loss\n",
    "\n",
    "For box regression tasks, it is common to use a combination of both L1 and L2 type losses. Most commonly the desired effect is to use an L1 loss early in training (when the loss values are large) and to transition to a smoother L2 loss as the algorithm converges. One such implementation of this smooth regression loss function is the Huber loss. \n",
    "\n",
    "![Huber Loss](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2022/notebooks/box_localization/pngs/huber_loss.png)\n",
    "\n",
    "More information can be found here: https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(y_true, y_pred, weights=1.0, delta=1.0, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to calculate mixed L1/L2 Huber loss\n",
    "    \n",
    "    \"\"\"\n",
    "    y_true = tf.expand_dims(y_true, axis=-1)\n",
    "    y_pred = tf.expand_dims(y_pred, axis=-1)\n",
    "    \n",
    "    loss = tf.keras.losses.huber(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        delta=delta)\n",
    "    \n",
    "    return tf.math.reduce_mean(loss * weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box metrics\n",
    "\n",
    "Finally, to keep track of classification performance, the use of the standard accuracy metric is suboptimal as the number of correct box predictions will quickly saturate to 100% (as the number of negative boxes >> number of positive boxes). In fact generally speaking, any metric that tracks performance of negative box predictions gtend not be very useful.\n",
    "\n",
    "Instead, consider the use of **sensitivity** and **PPV**:\n",
    "\n",
    "* sensitivity (recall): TP / (TP + FN)\n",
    "* positive predictive value (precision): TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_ce_sens(y_true, y_pred, weights=None, threshold=0.5, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement sensitivity (recall) on raw sigmoid (binary) cross-entropy logits\n",
    "\n",
    "    \"\"\"\n",
    "    p = tf.math.sigmoid(y_pred)\n",
    "    tp = (p > threshold) & (y_true > 0)\n",
    "    \n",
    "    if weights is not None:\n",
    "        tp = tp & (weights != 0)\n",
    "        y_true = (y_true > 0) & (weights != 0)\n",
    "\n",
    "    num = tf.math.count_nonzero(tp) \n",
    "    den = tf.math.count_nonzero(y_true)\n",
    "\n",
    "    num = tf.cast(num, tf.float32)\n",
    "    den = tf.cast(den, tf.float32)\n",
    "\n",
    "    return tf.math.divide_no_nan(num, den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_ce_ppv(y_true, y_pred, weights=None, threshold=0.5, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement PPV (precision) on raw sigmoid (binary) cross-entropy logits\n",
    "\n",
    "    \"\"\"\n",
    "    p = tf.math.sigmoid(y_pred)\n",
    "    pp = p > threshold\n",
    "    tp = pp & (y_true == 1)\n",
    "    \n",
    "    if weights is not None:\n",
    "        tp = tp & (weights != 0)\n",
    "        pp = pp & (weights != 0)\n",
    "\n",
    "    num = tf.math.count_nonzero(tp) \n",
    "    den = tf.math.count_nonzero(pp)\n",
    "\n",
    "    num = tf.cast(num, tf.float32)\n",
    "    den = tf.cast(den, tf.float32)\n",
    "    \n",
    "    return tf.math.divide_no_nan(num, den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked loss and metric functions\n",
    "\n",
    "As you recall, although boxes classification and regression ground truth values are calculated for **every box** per image, only a subset of boxes are used for algorithm training:\n",
    "\n",
    "* `cls` network: only boxes with IoU > 0.5 (positive) and IoU < 0.2 (negative)\n",
    "* `reg` network: only boxes that correspond to a positive classification\n",
    "\n",
    "To account for this, a sample weight mask is passed into the `weights` argument in each of the loss functions defined above. The relevant masking tensor is generated by the `bb.convert_box_to_msk(...)` method above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics\n",
    "\n",
    "To summarize, the following losses and metrics need to be defined:\n",
    "\n",
    "**Classification losses**: box prediction (presence or absence of box in each individual anchor map)\n",
    "\n",
    "* `cls-c3`: 12 x 12 grid of focal loss values (track **metrics** with *sensitivity* and *PPV*)\n",
    "* `cls-c4`: 6 x 6 grid of focal loss values (track **metrics** with *sensitivity* and *PPV*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# CLASSIFICATION\n",
    "# =============================================\n",
    "\n",
    "losses = {}\n",
    "metric = {}\n",
    "\n",
    "for key in ['cls-c3', 'cls-c4']:\n",
    "    \n",
    "    # --- Define focal loss\n",
    "    losses[key] = focal_sigmoid_ce(\n",
    "        y_true=inputs[key],\n",
    "        y_pred=logits[key],\n",
    "        weights=inputs[key + '-msk'])\n",
    "    \n",
    "    # --- Define sensitivity\n",
    "    metric[key + '-sen'] = sigmoid_ce_sens(\n",
    "        y_true=inputs[key],\n",
    "        y_pred=logits[key],\n",
    "        weights=inputs[key + '-msk'])\n",
    "    \n",
    "    # --- Define PPV\n",
    "    metric[key + '-ppv'] = sigmoid_ce_ppv(\n",
    "        y_true=inputs[key],\n",
    "        y_pred=logits[key],\n",
    "        weights=inputs[key + '-msk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression losses**: box refinement (fine-tuning of each positive box)\n",
    "\n",
    "* `reg-c3`: 12 x 12 grid of Huber (L1/L2) losses\n",
    "* `reg-c4`: 6 x 6 grid of Huber (L1/L2) losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# REGRESSION\n",
    "# =============================================\n",
    "\n",
    "for key in ['reg-c3', 'reg-c4']:\n",
    "    \n",
    "    # --- Define Huber loss\n",
    "    losses[key] = huber(\n",
    "        y_true=inputs[key],\n",
    "        y_pred=logits[key],\n",
    "        weights=inputs[key + '-msk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Now let us create the new wrapper model. The inputs are defined above already in our `inputs` Python dictionary. As outputs, let us return the tensors stored in the `logits` dict. We will name this new wrapper model `training` because it will be used for training only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the `loss` and `metric` tensors we defined above to the new `training` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add losses\n",
    "for key, l in losses.items():\n",
    "    training.add_loss(l)\n",
    "\n",
    "# --- Add metric\n",
    "for key, m in metric.items():\n",
    "    training.add_metric(m, name=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model\n",
    "\n",
    " To prepare the model for learning, a graph must be **compiled** with a strategy for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
    "\n",
    "# --- Compile model\n",
    "training.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory data\n",
    "\n",
    "For moderate sized datasets which are too large to fit into immediate hard-drive cache, but small enough to fit into RAM memory, it is often times a good idea to first load all training data into RAM memory for increased speed of training. The `client` can be used for this purpose as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory for faster training\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train model\n",
    "training.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=100, \n",
    "    epochs=20,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=100,\n",
    "    validation_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* use `BoundingBox` object to convert predictions to anchors or masks\n",
    "* compare prediction with ground-truth\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
    "test_train = box_generator(test_train)\n",
    "test_valid = box_generator(test_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: although the model is trained using 2D slices, there is nothing to preclude passing an entire 3D volume through the model at one time (e.g. consider that the entire 3D volume is a single *batch* of data). In fact, typically performance metrics for medical imaging models are commonly reported on a volume-by-volume basis (not slice-by-slice). Thus, use the `expand=True` flag in `client.create_generators(...)` as above to yield entire 3D volumes instead of slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run entire volume through model\n",
    "x = next(test_train)\n",
    "box = training.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating IoUs\n",
    "\n",
    "The logits are the raw predictions from the model, but to generate the corresponding boxes several post-processing steps are needed. First the positive boxes must be identified from the classification network. Then, the predicted template boxes need to be refined using the regression network:\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/regression.png)\n",
    "\n",
    "The `BoundingBox` object can be used to perform these steps:\n",
    "\n",
    "* `bb.convert_box_to_msk(...)`: convert box predictions into 3D mask (primarily visualization)\n",
    "* `bb.convert_box_to_anc(...)`: convert box predictions into anchors (`[y0, x0, y1, x1]`) (calculate IoUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert to anchors\n",
    "anchors, classes = bb.convert_box_to_anc(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: what do these anchors and classes represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the ground-truth overlays at the beginning of this tutorial, a number of boxes may be classified for each single ground-truth box. Accordinging, during inference a number of ground-truth boxes may be triggered. To prune this boxes, use **non-max suppression**, a technique that removes all boxes that above a certain IoU threshold with the highest scoring box. \n",
    "\n",
    "Test several different IoU thresholds to its effect on box outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show various boxes at different NMS thresholds\n",
    "msk = bb.convert_box_to_msk(box, iou_nms=0.2)\n",
    "imshow(x['dat'][0], msk[0], figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the specific prediction boxes have been isolated, compare them with the ground truth boxes using the `bb.calculate_ious(...)` method. This function will compare a given single box with a list of many ground-truth anchors. The maximum overlap generated represents the IoU value for the given prediction box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
    "test_train = box_generator(test_train)\n",
    "test_valid = box_generator(test_valid)\n",
    "\n",
    "ious = {\n",
    "    'med': [],\n",
    "    'p25': [],\n",
    "    'p75': []}\n",
    "\n",
    "for x in test_valid:\n",
    "    \n",
    "    # --- Predict\n",
    "    box = training.predict(x)\n",
    "        \n",
    "    # --- Convert predictions to anchors\n",
    "    anchors_pred, _ = bb.convert_box_to_anc(box, iou_nms=0.2)\n",
    "    \n",
    "    # --- Convert ground-truth to anchors\n",
    "    anchors_true, _ = bb.convert_box_to_anc(x)\n",
    "    \n",
    "    # --- Calculate IoUs\n",
    "    curr = []\n",
    "    for pred, true in zip(anchors_pred, anchors_true):\n",
    "        for p in pred:\n",
    "            iou = bb.calculate_ious(box=p, anchors=true)\n",
    "            if iou.size > 0:\n",
    "                curr.append(np.max(iou))\n",
    "            else: \n",
    "                curr.append(0)\n",
    "    \n",
    "    if len(curr) == 0:\n",
    "        curr = [0]\n",
    "        \n",
    "    ious['med'].append(np.median(curr))\n",
    "    ious['p25'].append(np.percentile(curr, 25))\n",
    "    ious['p75'].append(np.percentile(curr, 75))\n",
    "    \n",
    "ious = {k: np.array(v) for k, v in ious.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define columns\n",
    "df = pd.DataFrame(index=np.arange(ious['med'].size))\n",
    "df['iou_median'] = ious['med']\n",
    "df['iou_p-25th'] = ious['p25']\n",
    "df['iou_p-75th'] = ious['p75']\n",
    "\n",
    "# --- Print accuracy\n",
    "print(df['iou_median'].median())\n",
    "print(df['iou_p-25th'].median())\n",
    "print(df['iou_p-75th'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `backbone.save()` and `backbone.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "backbone.save('./model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del backbone\n",
    "backbone = models.load_model('./model.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Bounding box algorithms such as RetinaNet require careful tuning of many hyperparameters to define the distribution of template boxes and post-processing of model predictions. What is the effect of modifying these hyperparameters, and how do you choose good initial values for medical imaging problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "When defining a grid of template boxes, there are at six different hyperparameters to consider:\n",
    "\n",
    "```\n",
    "(iter)   c               : feature maps to use; c1 = 1st subsample, c2 = 2nd subsample, etc\n",
    "(iter)   anchor_shapes   : base shape of anchors in each feature map\n",
    "(iter)   anchor_scales   : scales of each anchor parameterized as 2 ** (i/3)\n",
    "(iter)   anchor_ratios   : aspect ratios of each anchor\n",
    "(float)  iou_upper       : upper IoU used for pos boxes\n",
    "(float)  iou_lower       : lower IoU used for neg boxes\n",
    "```\n",
    "\n",
    "These key hyperparameters dictate the behavior of the `BoundingBox` object. What happens when you modify any of these hyperparameter values? \n",
    "\n",
    "When calibrating a bounding box model, one factor to optimize is the total number and distribution of ground-truth bounding boxes for any given single target object. Because medical imaging problems tend to focus on sparse object detection (e.g., one or few number of objects per input), a good initial heuristic is to ensure at least 5-10+ template ground-truth boxes are available for each single target object. Additionally, it is computationally efficient to ensure that all feature map resolutions are used e.g., if you have one or several feature map resolutions that are never positive, then you should simply remove these resolutions from the box parameterization.\n",
    "\n",
    "How do we ensure that these criteria are met? Use the following code cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "During inference, it is important to carefully collapse near overlapping boxes in single discrete predictions using a non-max suppression technique. How does modification of the NMS threshold affect algorithm-generated box predictions? How does the total number of target objects and potential object overlap affect tuning of this parameter?\n",
    "\n",
    "Use the following code cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
