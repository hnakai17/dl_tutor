{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will introduce key concepts related to creating simple convolutional neural networks.\n",
    "\n",
    "**Convolutional Neural Networks**\n",
    "\n",
    "* convolutional operations\n",
    "* softmax cross-entroy loss\n",
    "* model training\n",
    "* evaluation\n",
    "* visualization\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow.keras import Input, Model, models, layers, losses, metrics, optimizers\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of the CIFAR-10 dataset comprising 10 different everyday objects (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/cifar`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='cifar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality. As needed, pass any custom configurations (e.g. batch size, normalization parameters, etc) into the optional `configs` dictionary argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "configs = {'batch': {'size': 36}}\n",
    "gen_train, gen_valid, client = datasets.prepare(name='cifar', configs=configs, custom_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield a total of `batch['size']` training samples based on the specified batch size. As before, each iteration yields dictionary of model inputs, `xs`. In the current example, there is just a single input image `xs['dat']` and a single target `xs['class']`. Let us examine the generator data: Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, _ = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "for k, v in xs.items():\n",
    "    print('key = {} : shape = {}'.format(k.ljust(7), v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example\n",
    "imshow(xs['dat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images\n",
    "imshow(xs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 36-element `xs['class']` vector corresponds to ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print xs['class']\n",
    "print(xs['class'].reshape(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Operations\n",
    "\n",
    "In this tutorial, a CNN will be created using 2D convolutional operations. Recall that convolutional operations are defined by the following minimum specifications:\n",
    "\n",
    "* filter / channel depth\n",
    "* kernel size\n",
    "* strides\n",
    "* padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first begin by defining a single `Input` variable `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define model input \n",
    "x = Input(shape=(32, 32, 3), dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply convolutional operations to this input tensor `x`, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define regular convolution\n",
    "l1 = layers.Conv2D(\n",
    "    filters=16, \n",
    "    kernel_size=(3, 3), \n",
    "    strides=(1, 1), \n",
    "    padding='same')(x)\n",
    "\n",
    "# --- Define strided convolution\n",
    "l1 = layers.Conv2D(\n",
    "    filters=16, \n",
    "    kernel_size=(3, 3), \n",
    "    strides=(2, 2), \n",
    "    padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse identical function arguments, consider maintaining a `kwargs` dictionary and pass using the `**` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# ---- Define stack of convolutions\n",
    "l1 = layers.Conv2D(filters=16, strides=(1, 1), **kwargs)(x)\n",
    "l2 = layers.Conv2D(filters=32, strides=(1, 1), **kwargs)(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks\n",
    "\n",
    "In addition to the requisite convolutional operationss and activation functions, batch normalization is almost universally used in modern CNN architectures. Thus at minimum, a common baseline *block* pattern of operations can be defined as:\n",
    "\n",
    "* convolutional operation\n",
    "* batch normalization\n",
    "* activation function (e.g. ReLU)\n",
    "\n",
    "Let us define a block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define block\n",
    "c1 = layers.Conv2D(filters=16, **kwargs)(x)\n",
    "n1 = layers.BatchNormalization()(c1)\n",
    "r1 = layers.ReLU()(n1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course of buildling CNNs, we will be writing **many** blocks over time. Thus for brevity, let us use lambda functions to define modular, reusable components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv2D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let us rewrite a block using lambda shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define block\n",
    "b1 = relu(norm(conv(x, 16, (1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the two most common **block patterns** will be regular convolutional block and a strided convolutional block (for subsampling). Let us then create two more high-level lambda functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see how easy it is to create series of alternating stride-1 and stride-2 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define series of blocks\n",
    "l1 = conv1(16, x)\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a series of convolutional block operations, recall that the CNN must transition to MLP type operations (e.g. matrix multiplications). To convert 3D (or 4D) feature maps into vectors, consider one of the following approaches:\n",
    "\n",
    "* serial convolutions (with stride > 1 or VALID type padding)\n",
    "* global pool operations (mean or max)\n",
    "* reshape / flatten operation\n",
    "\n",
    "The following line of code implements the reshape / flatten operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = layers.Flatten()(l4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other options are described in further detail in the tutorial exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now that the intermediate layer is defined as a vector, a number of standard MLP type operations may be performed, including creation of an arbitrary (optional) number of hidden layers. As before, the final layer should yield a vector of logit scores **without** any activation function applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add hidden layers if needed\n",
    "# .... (optional) ...\n",
    "\n",
    "# --- Final logit scores\n",
    "logits  = layers.Dense(10, name='class')(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models\n",
    "\n",
    "## Backbone model\n",
    "\n",
    "Using the concepts introduced above, let us start by recreating the base `backbone` model (e.g., without training dependencies). Use the following line of code to create the `backbone` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define input\n",
    "x = Input(shape=(32, 32, 3), dtype='float32')\n",
    "\n",
    "# --- Define series of blocks\n",
    "l1 = conv1(16, x)\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "\n",
    "# --- Flatten\n",
    "f0 = layers.Flatten()(l4)\n",
    "\n",
    "# --- Final logit scores\n",
    "logits  = layers.Dense(10, name='class')(f0)\n",
    "\n",
    "# --- Create model\n",
    "backbone = Model(inputs=x, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following line of code to pass a single batch of data through the created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict\n",
    "scores = backbone.predict(xs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output shape of the `scores` variable, what does this variable represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logit scores\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `backbone` model architecture is wrapped in a second model with additional layer(s) that define optimization behavior including loss function derivations.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "As before, we start by defining all `inputs` into our new *wrapper* model. In the first model, we only defined a single `x` input because the `backbone` base model is designed to accept any input value(s) and make predictions without requiring a target. In our second *wrapper* model, specifically designed to facilitate model training, we additionally need an `Input` object for the target `class` so that we can calculate a loss value and improve our model performance through optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'dat': Input(shape=(32, 32, 3), name='dat'),\n",
    "    'class': Input(shape=(1,), name='class')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `inputs` Python dictionary, let us first recreate the CNN model operations by **reusing** the `backbone` object that we already defined. Doing so means that our new *wrapper* model is explicitly derived from the `backbone`. Any updates applied to our new *wrapper* model are propogated to the `backbone` model and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define first step of new wrapper model\n",
    "logits = backbone(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross-entropy loss\n",
    "\n",
    "For a classification problem, the most common loss is defined by **softmax cross-entropy**. This loss in fact is composed of two seperate steps:\n",
    "\n",
    "* **softmax**: method to normalize logit scores into a probability distribution\n",
    "* **cross-entropy**: method to quantify difference between predicted distribution and true distribution\n",
    "\n",
    "To define a softmax cross-entropy loss in Tensorflow, use the `losses.SparseCategoricalCrossentropy(...)` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a loss object\n",
    "sce = losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned variable `sce` is a Python method that can be used to calculate softmax cross-entropy loss for a given set of logit scores (`y_pred`) and ground-truth (`y_true`). For demonstration, use the following lines of code to calculate the loss for the generate logit scores of the current data batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate loss\n",
    "sce(y_true=xs['class'], y_pred=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training proceeds, the goal of the algorithm is to update weights in a manner such that this loss value decreases over time.\n",
    "\n",
    "The `scores` variable represents the logit scores for one batch of input data. However, we still need to define a symbolic TensorFlow tensor as a part of our graph. Let us formally create this loss tensor now using the `sce` function using the `logits` returned by the `backbone` call earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create loss tensor\n",
    "loss = sce(y_true=inputs['class'], y_pred=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metric\n",
    "\n",
    "Compared to *loss* values which the model will directly optimize, a *metric* is a quantitative value that captures a more human-interpretable value for overall model performance. Because the metric does not contribute to the optimization process (only used to monitor performance), this value **does not** need to be differentiable.\n",
    "\n",
    "For a classification problem, the most common metric is overall *accuracy*. Use the TensorFlow built-in method `metrics.sparse_categorical_accuracy(...)` to calculate per-batch accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define accuracy\n",
    "acc = metrics.sparse_categorical_accuracy(y_true=inputs['class'], y_pred=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Now let us create the new wrapper model. The inputs are defined above already in our `inputs` Python dictionary. As outputs, let us return both the `logits` tensor as well as the `loss`. We will name this new wrapper model `training` because it will be used for training only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Model(inputs=inputs, outputs={'logits': logits, 'loss': loss, 'acc': acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the `loss` and `metric` tensorws we defined above to the new `training` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add loss\n",
    "training.add_loss(loss)\n",
    "\n",
    "# --- Add metric\n",
    "training.add_metric(acc, name='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a Model\n",
    "\n",
    "The current model parameters have been initialized to random values. Through exposure to data, the goal is for the model to *learn* optimal parameter values that allow for robust mapping of provided input to target output. To prepare the model for learning, a graph must be **compiled** with a strategy for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
    "\n",
    "# --- Compile model\n",
    "training.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Once the model has been compiled and the data prepared (via a generator), training can be invoked using the `training.fit(...)` method. Ensure that both the training and validation data generators are used. In this particular example, we are defining arbitrary epochs of 250 steps each. Training will proceed for 12 epochs in total. Validation statistics will be assess every fourth epoch. As needed, tune these arugments as need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=250, \n",
    "    epochs=12,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=250,\n",
    "    validation_freq=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `backbone.predict(...)` or `training.predict(...)` to obtain logit scores\n",
    "* use `np.argmax(...)` to obtain prediction\n",
    "* compare prediction with ground-truth\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to yield all examples from the valid generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate all examples\n",
    "xs = {'dat': [], 'class': []}\n",
    "\n",
    "for x, _ in test_valid:\n",
    "    xs['dat'].append(x['dat'])\n",
    "    xs['class'].append(x['class'])\n",
    "\n",
    "xs = {\n",
    "    'dat': np.concatenate(xs['dat']),\n",
    "    'class': np.concatenate(xs['class'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to run prediction through the **valid** cohort generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict\n",
    "outputs = training.predict(xs)\n",
    "\n",
    "# --- Argmax\n",
    "pred = np.argmax(outputs['logits'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare results in Pandas DataFrame for ease of analysis and sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=np.arange(pred.size))\n",
    "\n",
    "# --- Define columns\n",
    "df['true'] = xs['class']\n",
    "df['pred'] = pred\n",
    "df['corr'] = df['true'] == df['pred']\n",
    "\n",
    "# --- Print accuracy\n",
    "print(df['corr'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the `accuracy` calculated by `training.predict(...)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `backbone.save()` and `backbone.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "backbone.save('./model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del backbone\n",
    "backbone = models.load_model('./model.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Up until this point, this tutorial has presented a baseline of minimum requirements needed to successful train a CNN. The following exercises will assist in further optimization and customization of network architecture as well as hyperparameter considerations. Upon completion, apply any combination of techniques explored in this tutorial to finish the assignment: to train a CNN on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In addition a matrix flatten operation, conversion from 3D (or 4D) feature maps into vectors may be accomplished via:\n",
    "\n",
    "* serial convolutions (with stride > 1 or VALID type padding)\n",
    "* global pool operations (mean or max)\n",
    "\n",
    "Use the following cell to implement this two alternative methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define input\n",
    "x = Input(shape=(32, 32, 3), dtype='float32')\n",
    "\n",
    "# --- Define series of blocks\n",
    "l1 = conv1(16, x)\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "\n",
    "# --- Convert l4 to vector via serial convolution(s)\n",
    "op = layers.Conv2D(\n",
    "    filters=48, \n",
    "    kernel_size=?, \n",
    "    strides=?, \n",
    "    padding=?)\n",
    "\n",
    "vector = ?\n",
    "\n",
    "# --- Contert l4 to vector via global pool operations\n",
    "op = layers.?\n",
    "vector = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement via serial convolution(s), consider the size of the last feature map layer. Why is this size important? Accordiningly, what size must the convolutional filter be to accomplish the goal of generating a `1 x 1 x C` feature map? In addition to changes in convolutional `kernel_size`, how to modifications to `stride` and `padding` effect the output layer size?\n",
    "\n",
    "Use the following cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check l4 size\n",
    "l4.shape\n",
    "\n",
    "# --- Alter convolutional kernel size\n",
    "\n",
    "# --- Alter convolutional stride\n",
    "\n",
    "# --- Alter convolutional padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement via global pooling operations, consider referencing the Keras API here: https://keras.io/api/layers/pooling_layers/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit overfitting, consider use of an L2 regularizer. Recall that this technique adds an extra term to the loss function such that in addition to optimizing for classification accuracy, the model also will attempt to minimize overall L2 distance of all network parameters. \n",
    "\n",
    "In the Tensorflow 2 / Keras API, a `kernel_reguarlizer` can be specified in the definition of any layer operation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# --- Use regularizer\n",
    "op = layers.Conv2D(\n",
    "    filters=16, \n",
    "    kernel_size=(3, 3), \n",
    "    strides=(1, 1), \n",
    "    padding='same',\n",
    "    kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "l1 = relu(norm(op(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *lambda* parameter passed into the `regularizers.l2(...)` call is a constant value by which to scale this component of the loss; for more L2 regularization, use higher values. A *lambda* value of 0 is equivalent to no regularizer use at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other types of regularizers are present in the `regularizers.*` module? Can L1, L2 and/or both L1/L2 be used? What would be the relative benefit of some of these alternatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "To limit overfitting, consider use of Dropout. The `rate` argument passed during initialization determines the **fraction of input units to drop**. Thus a rate of 0.25 will retain 75% of activation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define dropout\n",
    "drop = layers.Dropout(rate=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the ReLU activation function, dropout can be applied either before or after the activation function (otherwise it is most common to apply dropout after the activation function). Also recall that dropout is most efficient in the MLP portions of your network. Finally just as the final matrix multiplication should not be defined with any activation function, ensure the **no dropout** is used in the final layer either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define series of blocks\n",
    "l1 = conv1(16, x)\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "\n",
    "# --- Flatten\n",
    "f0 = layers.Flatten()(l4)\n",
    "\n",
    "# --- Use dropout\n",
    "hidden_size = 64\n",
    "f1 = drop(layers.Dense(hidden_size, activation='relu')(f0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
