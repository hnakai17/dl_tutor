{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore several strategies to address class imbalance as well as how to tune a network with weighted loss functions (e.g. class weights and masks). Strategies discussed include:\n",
    "\n",
    "* stratified sampling\n",
    "* pixel-level class weights\n",
    "* pixel-level masked loss\n",
    "\n",
    "Ultimately, the goal of this tutorial (and class assignment) is to create a high sensitivity detector for kidney tumor on CT exams. \n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Tensorflow library version\n",
    "\n",
    "This tutorial will use the Tensorflow 2.1 library. Use the following line of code to select and download this specific version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download Tensorflow 2.x (only in Google Colab)\n",
    "% pip install tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, models, losses, layers, optimizers\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about he KiTS Challenge can be found here: https://kits21.kits-challenge.org/. In this exercise, we will use this dataset to derive a model for slice-by-slice kidney segmentation. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_kits`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='ct/kits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "\n",
    "The first strategy we explore to address class imbalance is stratified sampling e.g., we will increase the sampling frequency of slices with enhancing tumor to approximately 30%. More precisely, we will use the following sampling distribution:\n",
    "\n",
    "* class 0: 40% (background)\n",
    "* class 1: 30% (normal kidney)\n",
    "* class 2: 30% (tumor)\n",
    "\n",
    "To do so, we pass the appropriate `sampling` specifications to the `configs` variable when creating the data generators and the `Client()` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs dict\n",
    "configs = {\n",
    "    'batch': {'size': 16},\n",
    "    'sampling': {\n",
    "        'lbl-crp-00': 0.4,\n",
    "        'lbl-crp-01': 0.3,\n",
    "        'lbl-crp-02': 0.3}}\n",
    "\n",
    "# --- Prepare generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='2d', configs=configs, custom_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield a total of `batch['size']` training samples based on the specified batch size. As before, each iteration yields dictionary of model inputs, `xs`. In the current example, there is just a single input image `xs['dat']` and a single target `xs['lbl']`. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, _ = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "for k, v in xs.items():\n",
    "    print('key = {} : shape = {}'.format(k.ljust(7), v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD5ejoTbx1_0"
   },
   "source": [
    "### KITS Data\n",
    "\n",
    "The input images in the variable `dat` are matrices of shape `1 x 96 x 96 x 1`. Note that even though the images here are 2D in shape, the full matrix is a 3D tensor `(z, y, x)` where `z = 1` in this implementation. Note that although the 3rd z-axis dimension is redundant here (for a single slice input), many of our more complex models and architectures will commonly require a full 3D tensor. Because of this, we will directly use 3D convolutions throughout the tutorial materials for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example\n",
    "imshow(xs['dat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images\n",
    "imshow(xs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney masks\n",
    "\n",
    "The ground-truth labels are three class masks of the same matrix shape as the model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs['lbl'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `imshow(...)` method to visualize the ground-truth tumor mask labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show tumor masks overlaid on original data\n",
    "imshow(xs['dat'], xs['lbl'])\n",
    "\n",
    "# --- Show tumor masks isolated\n",
    "imshow(xs['lbl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "To localize enhancing tumor on brain MRI, we will implement a standard contracting-expanding network (e.g. U-Net). In the assignment, feel free to try various architecture permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)\n",
    "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "\n",
    "concat = lambda a, b : layers.Concatenate()([a, b])\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input input\n",
    "x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
    "\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, x)\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "\n",
    "# --- Define expanding layers\n",
    "l6  = tran2(48, l5)\n",
    "l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
    "l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
    "l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
    "l10 = conv1(8,  l9)\n",
    "\n",
    "# --- Create logits\n",
    "logits = layers.Conv3D(filters=2, **kwargs)(l10)\n",
    "\n",
    "# --- Create model\n",
    "backbone = Model(inputs=x, outputs=logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `backbone` model architecture is wrapped in a second model with additional layer(s) that define optimization behavior including loss function derivations.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "As before, we start by defining all `inputs` into our new *wrapper* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create inputs\n",
    "inputs = {\n",
    "    'dat': Input(shape=(None, 96, 96, 1), dtype='float32'),\n",
    "    'lbl': Input(shape=(None, 96, 96, 1), dtype='uint8')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `inputs` Python dictionary, let us first recreate the CNN model operations by **reusing** the `backbone` object that we already defined. Doing so means that our new *wrapper* model is explicitly derived from the `backbone`. Any updates applied to our new *wrapper* model are propogated to the `backbone` model and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define first step of new wrapper model\n",
    "logits = backbone(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Loss\n",
    "\n",
    "To implement custom loss weights (and/or masks), create a `wgt` array to perform a point-wise multiplication against the final pixel-by-pixel loss. For locations where the loss should be **weighted**, use a constant value > 1, For locations where the loss should be ignored (**masked**), use a constant value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "There three different custom class weight `wgt` tensors that will be explored in this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 1**: Use class weights to increase the penalty for enhancing tumor voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(lbl, pos_weight=3.0):\n",
    "    \n",
    "    # --- Create wgt\n",
    "    wgt = tf.ones_like(lbl, dtype='float32')\n",
    "    wgt = wgt + tf.cast(lbl == 2, dtype='float32') * (pos_weight - 1.0)\n",
    "    \n",
    "    return wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 2**: Use a masked loss function to ignore the contribution of non-tumor voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(lbl, pos_weight=3.0):\n",
    "    \n",
    "    # --- Create wgt\n",
    "    wgt = tf.cast(lbl > 0, dtype='float32')\n",
    "    \n",
    "    return wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 3**: Use a combination of both class weights and masked losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(lbl, pos_weight=3.0):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the `wgt` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create weights\n",
    "wgt = create_weights(inputs['lbl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target ground-truth\n",
    "\n",
    "Additionally, while our ground-truth `inputs['lbl']` currently comprises of three separate classes (background, kidney, tumor), we will be predicting a binary segmentation output of tumor vs. no tumor only. Thus in addition to creating a `wgt` array, we will be creating an additional binarized `y_true` ground-truth array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create lbl\n",
    "y_true = tf.cast(inputs['lbl'] > 0, dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model\n",
    "\n",
    "To compile this model, several custom `loss` and `metrics` objects will need to be defined.\n",
    "\n",
    "#### Loss\n",
    "\n",
    "As in prior tutorials, a standard (sparse) softmax cross-entropy loss will be used to optimize the segmentation model.\n",
    "\n",
    "```python\n",
    "def sce(weights, scale=1.0):\n",
    "\n",
    "    loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def sce(y_true, y_pred):\n",
    "\n",
    "        return loss(y_true=y_true, y_pred=y_pred, sample_weight=weights) * scale\n",
    "\n",
    "    return sce \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create sce\n",
    "sce = losses.SparseCategoricalCrossentropy(from_logits=True)(\n",
    "    y_true=lbl,\n",
    "    y_pred=logits,\n",
    "    sample_weight=wgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "For class imbalanced datasets, Dice score may be a limited evaluation metric. Thus we will additionally use foreground sensitivity as an additional value to track overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dsc(y_true, y_pred, weights=None, c=1):\n",
    "    \"\"\"\n",
    "    Method to calculate the Dice score coefficient for given class\n",
    "    \n",
    "    :params\n",
    "    \n",
    "      y_true : ground-truth label\n",
    "      y_pred : predicted logits scores\n",
    "           c : class to calculate DSC on\n",
    "    \n",
    "    \"\"\"    \n",
    "    true = y_true[..., 0] == c\n",
    "    pred = tf.math.argmax(y_pred, axis=-1) == c \n",
    "    \n",
    "    if weights is not None:\n",
    "        true = true & (weights[..., 0] != 0)\n",
    "        pred = pred & (weights[..., 0] != 0)\n",
    "\n",
    "    A = tf.math.count_nonzero(true & pred) * 2\n",
    "    B = tf.math.count_nonzero(true) + tf.math.count_nonzero(pred)\n",
    "    \n",
    "    return tf.math.divide_no_nan(\n",
    "        tf.cast(A, tf.float32), \n",
    "        tf.cast(B, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sen(y_true, y_pred, weights=None, c=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement sensitivity (recall) on raw sigmoid (binary) cross-entropy logits\n",
    "\n",
    "    \"\"\"\n",
    "    true = y_true[..., 0] == c\n",
    "    pred = tf.math.argmax(y_pred, axis=-1) == c \n",
    "    \n",
    "    if weights is not None:\n",
    "        true = true & (weights[..., 0] != 0)\n",
    "        pred = pred & (weights[..., 0] != 0)\n",
    "        \n",
    "    tp = true & pred\n",
    "\n",
    "    num = tf.math.count_nonzero(tp) \n",
    "    den = tf.math.count_nonzero(y_true)\n",
    "\n",
    "    num = tf.cast(num, tf.float32)\n",
    "    den = tf.cast(den, tf.float32)\n",
    "\n",
    "    return tf.math.divide_no_nan(num, den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create metrics\n",
    "dsc = calculate_dsc(y_true, logits, wgt)\n",
    "sen = calculate_sen(y_true, logits, wgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Now let us create the new wrapper model. The inputs are defined above already in our `inputs` Python dictionary. We will name this new wrapper model `training` because it will be used for training only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Model(inputs=inputs, outputs={'logits': logits, 'dsc': dsc, 'sen': sen})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the `loss` and `metric` tensors we defined above to the new `training` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add losses\n",
    "training.add_loss(sce)\n",
    "\n",
    "# --- Add metric\n",
    "training.add_metric(dsc, name='dsc')\n",
    "training.add_metric(sen, name='sen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model\n",
    "\n",
    " To prepare the model for learning, a graph must be **compiled** with a strategy for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
    "\n",
    "# --- Compile model\n",
    "training.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory data\n",
    "\n",
    "For moderate sized datasets which are too large to fit into immediate hard-drive cache, but small enough to fit into RAM memory, it is often times a good idea to first load all training data into RAM memory for increased speed of training. The `client` can be used for this purpose as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory for faster training\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "To use Tensorboard, create the necessary Keras callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks  \n",
    "tensorboard_callback = callbacks.TensorBoard('./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train model\n",
    "training.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=100, \n",
    "    epochs=10,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=100,\n",
    "    validation_freq=5,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Tensorboard\n",
    "\n",
    "After running several iterations, start Tensorboard using the following cells. After Tensorboard has registered the first several checkpoints, subsequent data will be updated automatically (asynchronously) and model training can be resumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext tensorboard\n",
    "% tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* compare prediction with ground-truth (Dice score, sensitivity)\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run prediction on a single (first) example from the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run a single prediction\n",
    "x, y = next(test_valid)\n",
    "logits = training.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the predicted results. Recall that the `np.argmax(...)` function can be used to convert raw logit scores to predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create prediction\n",
    "pred = np.argmax(logits[0], axis=-1)\n",
    "\n",
    "# --- Show\n",
    "imshow(x['dat'][0, ..., 0], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint** What is the problem with this mask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that during training, the algorithm is never penalized regardless of class for predictions *outside of the mask* (e.g. values == 0) used for training. Thus, to generate the final prediction, one needs to similarly remove the masked values of the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean up pred using mask\n",
    "pred[x['msk'][0, ..., 0] == 0] = 0\n",
    "\n",
    "# --- Show\n",
    "imshow(x['dat'][0, ..., 0], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better. Let us look at the ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show\n",
    "imshow(x['dat'][0], y['tumor'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for sensitivity\n",
    "\n",
    "In addition to evaluating overall model Dice score, the goal of this exercise is to create a high-sensitivity model. Recall that sensitivity is defined as the number of TP predictions / all positive exams (e.g. proportion of positive findings that are correctly identified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sens(pred, true, epsilon=1):\n",
    "    \"\"\"\n",
    "    Method to calculate sensitivity from pred and true masks\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate sens\n",
    "calculate_sens(\n",
    "    pred=pred,\n",
    "    true=y['tumor'][0, ..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)\n",
    "\n",
    "for x, y in test_valid:\n",
    "    \n",
    "    # --- Create prediction\n",
    "    pred = np.argmax(logits[0], axis=-1)\n",
    "    \n",
    "    # --- Clean up pred using mask\n",
    "    pred[x['msk'][0, ..., 0] == 0] = 0\n",
    "    \n",
    "    # --- Calculate Dice\n",
    "    dice = ...\n",
    "    \n",
    "    # --- Calculate sens\n",
    "    sens = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define columns\n",
    "df = pd.DataFrame(...)\n",
    "df['dice'] = ...\n",
    "df['sens'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./class_imbalance.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./class_imbalance.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In this tutorial, we presented a general strategy for creating custom loss functions using the Tensorflow / Keras API with a focus on strategies for class imbalance. In addition to weighted and masked loss functions, two other alternative (often complimentary) approaches include the **focal loss** and **soft Dice loss**. While these strategies are not required for the weekly homework assignment, these optional exercises will provide you an opportunity to become familiar with creating and customizing your own loss functions in Tensorflow / Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise, create a custom implementation of the focal loss. As you recall, the **focal loss** function gradually titrates the contribution of any given prediction such that more confident correct predictions over time become weighted less than incorrect predictions:\n",
    "\n",
    "```\n",
    "focal loss = -(1 - p[t]) ** y * alpha * log(p[t])\n",
    "\n",
    "```\n",
    "Where:\n",
    "\n",
    "* `p[t]`: sigmoid transformation of raw logit prediction on between `[0, 1]`\n",
    "* `alpha`: weight for cross-entropy loss\n",
    "* `y`: gamma value for focal loss (y == 0 for no focal loss, y > 1 for more focal loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_sigmoid_ce(weights=1.0, scale=1.0, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Method to implement focal sigmoid (binary) cross-entropy loss\n",
    "\n",
    "    \"\"\"\n",
    "    def focal_sigmoid_ce(y_true, y_pred):\n",
    "\n",
    "        # --- Calculate standard cross entropy with alpha weighting\n",
    "        loss = ...\n",
    "\n",
    "        # --- Calculate modulation to pos and neg labels \n",
    "        p = ...\n",
    "        modulation_pos = ...\n",
    "        modulation_neg = ...\n",
    "\n",
    "        mask = tf.dtypes.cast(y_true, dtype=tf.bool)\n",
    "        modulation = tf.where(mask, modulation_pos, modulation_neg)\n",
    "\n",
    "        return tf.math.reduce_sum(modulation * loss * weights * scale)\n",
    "\n",
    "    return focal_sigmoid_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Consider the following two-step implementation approach:\n",
    "\n",
    "1. `alpha * log(p[t])`: This term is essentially the same as the standard weighted cross entroy loss. Is there a Tensorflow / Keras loss function that we can use to implement this component of the focal loss function?\n",
    "\n",
    "2. `(1 - p[t]) ** y`: This term requires scaling or modulating the loss by the *confidence* of prediction. For the positive class, probablity values close to 1.0 indicate confident, correct predictions. For the negative class, probability values close to 0.0 indicate confident, correct predictions. Consider calcuating the modulation scaling factor for both positive and negative classes separately, and using `tf.where(...)` to combine the two tensors based on ground-truth. Also don't forget to convert `y_pred` (logits) to probability using a sigmoid transform using `tf.math.sigmoid(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In this exercise, we will modify the standard Dice score function to allow for differentiation. Recall that the Dice score is calculated as follows:\n",
    "\n",
    "* numerator: overlap of A and B (multiplied by 2)\n",
    "* denominator: |A| + |B|\n",
    "\n",
    "For modification, we will replace the overlap with a multiplication by the **softmax** normalization of the logits and the ground-truth mask. In addition, we will replace the `|A|` with the total sum of the **softmax** normalized logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsc_soft(weights=None, scale=1.0, epsilon=1, cls=1):\n",
    "    \"\"\"\n",
    "    Method for soft (differentiable) Dice score calculation\n",
    "\n",
    "    :params\n",
    "\n",
    "      (int) cls : class to use to for Dice score calculation (default = 1)\n",
    "\n",
    "    \"\"\"\n",
    "    def calc_dsc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Method to calculate Dice coefficient\n",
    "\n",
    "        \"\"\"\n",
    "        true = tf.cast(y_true[..., 0] == cls, tf.float32)\n",
    "        pred = ...\n",
    "\n",
    "        if weights is not None:\n",
    "            true = true * (weights[..., 0]) \n",
    "            pred = pred * (weights[..., 0])\n",
    "\n",
    "        num = ...\n",
    "        den = ...\n",
    "\n",
    "        return -(num / den) * scale\n",
    "\n",
    "    return calc_dsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: While the ground-truth masks may remain binary, to ensure a differentiable function, the **softmax** transform of the logits is used for the prediction tensor. To implement this loss properly, ensure that the `pred` tensor above accounts for the softmax normalization and that the appropriate modifications are made to both the numerator and denominator of the Dice score calculation above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
