{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will create a model for deep clustering as a pretext task for kidney segmentation.\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss library\n",
    "\n",
    "To facilitate fast kmeans clustering, we will use an efficient algorithm implemented by the Facebook AI Research team as part of the `faiss` library. In brief, `faiss` is a library for efficient similarity search and clustering of dense vectors. More information can be found here: https://github.com/facebookresearch/faiss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install faiss\n",
    "% pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, losses, metrics, layers, optimizers\n",
    "import faiss\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils import io\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about he KiTS Challenge can be found here: https://kits21.kits-challenge.org/. In this exercise, we will use this dataset to derive a model for slice-by-slice kidney segmentation. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_kits`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='ct/kits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    "In this tutorial, only the middle 2D slice of each volume will be used to promote fast model convergence. Since this small dataset fits easily into RAM memory, the following code block may be used to load these slices into a single Numpy array. Preparing data in this manner will also facilitate rapid iteration including efficient dataset clustering during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label=1, flip=True, a_min=-128, a_max=256):\n",
    "\n",
    "    # --- Create data client\n",
    "    _, _, client = datasets.prepare(name='ct/kits', keyword='3d')\n",
    "\n",
    "    dats, lbls = [], []\n",
    "\n",
    "    for sid, fnames, header in client.db.cursor():\n",
    "\n",
    "        lbl, _ = io.load(fnames['lbl-crp'])\n",
    "        \n",
    "        if label in lbl:\n",
    "            \n",
    "            dat, _ = io.load(fnames['dat-crp'])\n",
    "            dats.append(dat[48:49])\n",
    "            lbls.append(lbl[48:49] >= label)\n",
    "\n",
    "            if header['cohort-left'] and flip:\n",
    "                dats[-1]= dats[-1][..., ::-1, :]\n",
    "                lbls[-1]= lbls[-1][..., ::-1, :]\n",
    "\n",
    "    dats = np.stack(dats, axis=0)\n",
    "    lbls = np.stack(lbls, axis=0)\n",
    "    \n",
    "    # --- Nomralize dats\n",
    "    dats = (dats - a_min) / (a_max - a_min)\n",
    "    dats = dats.clip(min=0, max=1)\n",
    "\n",
    "    return {'dat': dats, 'lbl': lbls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data\n",
    "xs = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD5ejoTbx1_0"
   },
   "source": [
    "### KITS Data\n",
    "\n",
    "The input images in the variable `dat` are matrices of shape `1 x 96 x 96 x 1`. Note that even though the images here are 2D in shape, the full matrix is a 3D tensor `(z, y, x)` where `z = 1` in this implementation. Note that although the 3rd z-axis dimension is redundant here (for a single slice input), more complex models and architectures will commonly require a full 3D tensor. Because of this, we will directly use 3D convolutions throughout the tutorial materials for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example\n",
    "imshow(xs['dat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of 16 images\n",
    "imshow(xs['dat'][:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney masks\n",
    "\n",
    "The ground-truth labels are two class masks of the same matrix shape as the model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs['lbl'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three classes represent:\n",
    "\n",
    "* class 0: background\n",
    "* class 1: kidney\n",
    "\n",
    "Use the `imshow(...)` method to visualize the ground-truth tumor mask labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show tumor masks overlaid on original data\n",
    "imshow(xs['dat'][:16], xs['lbl'][:16])\n",
    "\n",
    "# --- Show tumor masks isolated\n",
    "imshow(xs['lbl'][:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(x, x_weight=1, x_blur=3, coords_weight=1., backbone=None, backbone_weight=1., **kwargs):\n",
    "    \"\"\"\n",
    "    Method to construct feature vector for clustering\n",
    "    \n",
    "    \"\"\"\n",
    "    x_ = [] \n",
    "\n",
    "    # --- Use features from raw data voxels\n",
    "    if x_weight > 0:\n",
    "        xx = x.copy()\n",
    "        if x_blur > 0:\n",
    "            xx[:, 0] = ndimage.gaussian_filter(xx[:, 0], sigma=(0, x_blur, x_blur, 0))\n",
    "        x_.append(xx * x_weight)\n",
    "\n",
    "    # --- Use features from coordinate location\n",
    "    if coords_weight > 0:\n",
    "        ij = np.meshgrid(*tuple([np.linspace(0, 1, 96) for _ in range(2)]), indexing='ij')\n",
    "        ij = np.expand_dims(np.stack(ij, axis=-1), axis=0)\n",
    "        ij = np.stack([ij] * x.shape[0], axis=0)\n",
    "        x_.append(ij * coords_weight)\n",
    "\n",
    "    # --- Use features from CNN-derived backbone\n",
    "    if backbone is not None:\n",
    "        yy = backbone.predict(x)\n",
    "        x_.append(yy * backbone_weight)\n",
    "\n",
    "    return np.concatenate(x_, axis=-1).reshape(x.size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(x, n_clusters=8, **kwargs):\n",
    "\n",
    "    # --- Create features\n",
    "    x_ = create_features(x=x, **kwargs)\n",
    "\n",
    "    # --- Apply kmeans clustering\n",
    "    kmeans = faiss.Kmeans(x_.shape[-1], n_clusters)\n",
    "    kmeans.train(x_.astype('float32'))\n",
    "    clusters = kmeans.index.search(x_.astype('float32'), 1)[1].reshape(x.shape)\n",
    "\n",
    "    return kmeans, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans, clusters = create_clusters(x=xs['dat'], n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(xs['dat'][:16], clusters[:16])\n",
    "imshow(clusters[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "For this task, we will implement a standard contracting-expanding network for semantic segmentation (e.g. U-Net). In the assignment, feel free to try various architecture permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create backbone\n",
    "\n",
    "Define standard lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)\n",
    "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "\n",
    "concat = lambda a, b : layers.Concatenate()([a, b])\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define standard U-Net backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet():\n",
    "    \n",
    "    # --- Input input\n",
    "    x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
    "\n",
    "    # --- Define contracting layers\n",
    "    l1 = conv1(8, x)\n",
    "    l2 = conv1(16, conv2(16, l1))\n",
    "    l3 = conv1(32, conv2(32, l2))\n",
    "    l4 = conv1(48, conv2(48, l3))\n",
    "    l5 = conv1(64, conv2(64, l4))\n",
    "\n",
    "    # --- Define expanding layers\n",
    "    l6  = tran2(48, l5)\n",
    "    l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
    "    l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
    "    l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
    "    l10 = conv1(8,  l9)\n",
    "\n",
    "    # --- Create embedding\n",
    "    outputs = layers.Conv3D(filters=8, **kwargs)(l10)\n",
    "\n",
    "    # --- Create model\n",
    "    backbone = Model(inputs=x, outputs=outputs) \n",
    "    \n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `backbone` model, a total of two separate `training` models need to be created:\n",
    "\n",
    "* training for deep clustering pretext task (pretraining)\n",
    "* training for kidney segmentation task (fine-tuning)\n",
    "\n",
    "For both `training` models, the `backbone` model architecture is wrapped in a second model with additional layer(s) that define optimization behavior including loss function derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(use_augmentation=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create generic model inputs (for pretraining and fine-tuning)\n",
    "    \n",
    "    \"\"\"\n",
    "    x = Input(shape=(None, 96, 96, 1))\n",
    "    y = Input(shape=(None, 96, 96, 1))\n",
    "\n",
    "    inputs = {'x': x, 'y': y}\n",
    "\n",
    "    # --- Data augmentation\n",
    "    if use_augmentation:\n",
    "        \n",
    "        a = layers.Concatenate()((inputs['x'][:, 0], inputs['y'][:, 0]))\n",
    "        a = layers.experimental.preprocessing.RandomRotation(factor=0.2, interpolation='nearest')(a)\n",
    "        a = layers.experimental.preprocessing.RandomTranslation(0.2, 0.2, interpolation='nearest')(a)\n",
    "        a = layers.experimental.preprocessing.RandomZoom(0.2, interpolation='nearest')(a)\n",
    "        a = tf.expand_dims(a, axis=1)\n",
    "\n",
    "        x = a[..., 0:1]\n",
    "        y = a[..., 1:2]\n",
    "        \n",
    "    return inputs, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dsc(y_true, y_pred, cls=1):\n",
    "    \"\"\" \n",
    "    Method to calculate Dice score for given class\n",
    "    \n",
    "    \"\"\"\n",
    "    true = y_true[..., 0] == cls\n",
    "    pred = tf.math.argmax(y_pred, axis=-1) == cls\n",
    "\n",
    "    A = tf.math.count_nonzero(true & pred) * 2\n",
    "    B = tf.math.count_nonzero(true) + tf.math.count_nonzero(pred)\n",
    "\n",
    "    return tf.math.divide_no_nan(tf.cast(A, tf.float32), tf.cast(B, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(training, y, outputs, use_dsc=False, **kwargs):\n",
    "\n",
    "    sce = losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=y, y_pred=outputs)\n",
    "    acc = metrics.sparse_categorical_accuracy(y_true=y, y_pred=outputs)\n",
    "\n",
    "    training.add_loss(sce)\n",
    "    training.add_metric(acc, 'acc')\n",
    "\n",
    "    if use_dsc:\n",
    "        dsc = calculate_dsc(y_true=y, y_pred=outputs)\n",
    "        training.add_metric(dsc, 'dsc')\n",
    "\n",
    "    training.compile(optimizer=optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(xs, clusters, backbone=None, epochs=50, batch_size=15, **kwargs):\n",
    "\n",
    "    # --- Create inputs\n",
    "    inputs, x, y = create_inputs(**kwargs)\n",
    "\n",
    "    # --- Create backbone (unet)\n",
    "    if backbone is None:\n",
    "        backbone = create_unet()\n",
    "\n",
    "    # --- Create training outputs\n",
    "    outputs = backbone(x)\n",
    "    outputs = layers.Conv3D(kernel_size=1, filters=(clusters.max() + 1))(outputs)\n",
    "    \n",
    "    # --- Create training model and losses\n",
    "    training = Model(inputs=inputs, outputs=outputs)\n",
    "    training = compile_model(training, y, outputs)\n",
    "\n",
    "    # --- Train\n",
    "    training.fit(x={'x': xs['dat'], 'y': clusters}, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    return backbone, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone, training = pretrain(xs, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(xs, backbone=None, N=10, epochs=500, batch_size=15, validation_freq=50, **kwargs): \n",
    "\n",
    "    # --- Create inputs\n",
    "    inputs, x, y = create_inputs(**kwargs)\n",
    "\n",
    "    # --- Create backbone (unet)\n",
    "    if backbone is None:\n",
    "        backbone = create_unet()\n",
    "\n",
    "    # --- Create training outputs\n",
    "    outputs = backbone(x)\n",
    "    outputs = layers.Conv3D(kernel_size=1, filters=2)(outputs)\n",
    "    \n",
    "    # --- Create training model and losses\n",
    "    training = Model(inputs=inputs, outputs=outputs)\n",
    "    training = compile_model(training, y, outputs, use_dsc=True)\n",
    "\n",
    "    # --- Train\n",
    "    training.fit(\n",
    "        x={'x': xs['dat'][:N], 'y': xs['lbl'][:N]}, \n",
    "        validation_data={'x': xs['dat'][N:], 'y': xs['lbl'][N:]}, \n",
    "        validation_freq=validation_freq,\n",
    "        batch_size=max(N, batch_size),\n",
    "        epochs=epochs)\n",
    "\n",
    "    return backbone, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone, training = finetune(xs, backbone=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(xs, n_clusters=8, epochs=3, pretrain_epochs=50, finetune_epochs=500, N_finetune=10):\n",
    "\n",
    "    backbone = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print('==================================================================')\n",
    "        print('STARTING EPOCH: {}'.format(epoch + 1))\n",
    "        print('==================================================================')\n",
    "\n",
    "        # --- Create clusters\n",
    "        kwargs = {\n",
    "            'backbone': backbone, \n",
    "            'x_weight': 1. if epoch == 0 else 0, \n",
    "            'coords_weight': 1. if epoch == 0 else 0}\n",
    "\n",
    "        kmeans, clusters = create_clusters(x=xs['dat'], n_clusters=n_clusters, **kwargs)\n",
    "\n",
    "        # --- Perform pretraining\n",
    "        backbone, training = pretrain(xs=xs, clusters=clusters, backbone=backbone, epochs=pretrain_epochs)\n",
    "\n",
    "    # --- Perform fine-tuning\n",
    "    backbone, training = finetune(xs=xs, clusters=clusters, N=N_finetune, epochs=finetune_epochs)\n",
    "    \n",
    "    return backbone, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone, training = run_experiment(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run prediction\n",
    "logits = training.predict({'x': xs['dat'], 'y': xs['lbl']})\n",
    "\n",
    "dice = []\n",
    "for y_true, y_pred in zip(xs['lbl'], logits):\n",
    "    \n",
    "    dsc = calculate_dsc(y_true=y_true, y_pred=y_pred).numpy()\n",
    "    dice.append(dsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define columns\n",
    "df = pd.DataFrame(np.arange(len(dice)))\n",
    "df['dice'] = dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `training.save()` and `training.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "training.save('./model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del training\n",
    "training = models.load_model('./model.hdf5', compile=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
