{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment we will create a model for segmentation of tumor from abdominal CT images using custom loss function modifications to increase prediction sensitivity.\n",
    "\n",
    "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Once complete, the following items must be submitted:\n",
    "\n",
    "* final `*.ipynb` notebook\n",
    "* final trained `*.hdf5` model file\n",
    "* final compiled `*.csv` file with performance statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss library\n",
    "\n",
    "To facilitate fast kmeans clustering, we will use an efficient algorithm implemented by the Facebook AI Research team as part of the `faiss` library. In brief, `faiss` is a library for efficient similarity search and clustering of dense vectors. More information can be found here: https://github.com/facebookresearch/faiss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install faiss\n",
    "% pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, models, losses, metrics, layers, optimizers\n",
    "import faiss\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils import io\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about he KiTS Challenge can be found here: https://kits21.kits-challenge.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='ct/kits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    "In this assignment, only the middle 2D slice of each volume will be used to promote fast model convergence. Since this small dataset fits easily into RAM memory, the following code block may be used to load these slices into a single Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label=1, flip=True, a_min=-128, a_max=256):\n",
    "\n",
    "    # --- Create data client\n",
    "    _, _, client = datasets.prepare(name='ct/kits', keyword='3d')\n",
    "\n",
    "    dats, lbls = [], []\n",
    "\n",
    "    for sid, fnames, header in client.db.cursor():\n",
    "\n",
    "        lbl, _ = io.load(fnames['lbl-crp'])\n",
    "        \n",
    "        if label in lbl:\n",
    "            \n",
    "            dat, _ = io.load(fnames['dat-crp'])\n",
    "            dats.append(dat[48:49])\n",
    "            lbls.append(lbl[48:49] >= label)\n",
    "\n",
    "            if header['cohort-left'] and flip:\n",
    "                dats[-1]= dats[-1][..., ::-1, :]\n",
    "                lbls[-1]= lbls[-1][..., ::-1, :]\n",
    "\n",
    "    dats = np.stack(dats, axis=0)\n",
    "    lbls = np.stack(lbls, axis=0)\n",
    "    \n",
    "    # --- Nomralize dats\n",
    "    dats = (dats - a_min) / (a_max - a_min)\n",
    "    dats = dats.clip(min=0, max=1)\n",
    "\n",
    "    return {'dat': dats, 'lbl': lbls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell to load all data into the `xs` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data\n",
    "xs = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters\n",
    "\n",
    "To create useful clusters for semantic segmentation, consider the following potential features:\n",
    "\n",
    "* pixel (voxel) value\n",
    "* pixel (voxel) coordinate location\n",
    "* CNN-derived features from algorithm training\n",
    "\n",
    "The following block can be used to create a pixel-wise (voxel-wise) feature vector based on various permuatations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(x, x_weight=1, x_blur=3, coords_weight=1., backbone=None, backbone_weight=1., **kwargs):\n",
    "    \"\"\"\n",
    "    Method to construct feature vector for clustering\n",
    "    \n",
    "    \"\"\"\n",
    "    x_ = [] \n",
    "\n",
    "    # --- Use features from raw data voxels\n",
    "    if x_weight > 0:\n",
    "        xx = x.copy()\n",
    "        if x_blur > 0:\n",
    "            xx[:, 0] = ndimage.gaussian_filter(xx[:, 0], sigma=(0, x_blur, x_blur, 0))\n",
    "        x_.append(xx * x_weight)\n",
    "\n",
    "    # --- Use features from coordinate location\n",
    "    if coords_weight > 0:\n",
    "        ij = np.meshgrid(*tuple([np.linspace(0, 1, 96) for _ in range(2)]), indexing='ij')\n",
    "        ij = np.expand_dims(np.stack(ij, axis=-1), axis=0)\n",
    "        ij = np.stack([ij] * x.shape[0], axis=0)\n",
    "        x_.append(ij * coords_weight)\n",
    "\n",
    "    # --- Use features from CNN-derived backbone\n",
    "    if backbone is not None:\n",
    "        yy = backbone.predict(x)\n",
    "        x_.append(yy * backbone_weight)\n",
    "\n",
    "    return np.concatenate(x_, axis=-1).reshape(x.size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chosen feature feature combination, perform clustering using the `faiss` library. The following method creates a total of `n_clusters` from the input data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(x, n_clusters=8, **kwargs):\n",
    "\n",
    "    # --- Create features\n",
    "    x_ = create_features(x=x, **kwargs)\n",
    "\n",
    "    # --- Apply kmeans clustering\n",
    "    kmeans = faiss.Kmeans(x_.shape[-1], n_clusters)\n",
    "    kmeans.train(x_.astype('float32'))\n",
    "    clusters = kmeans.index.search(x_.astype('float32'), 1)[1].reshape(x.shape)\n",
    "\n",
    "    return kmeans, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: These code blocks for clustering do not need to be modified for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the backbone model\n",
    "\n",
    "Use the following cell block to define your backbone for the semantic segmentation task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define shared methods\n",
    "\n",
    "Use the following cell block to define your shared methods for the pretraining and fine-tuning models. Consider the following shared components may be defined:\n",
    "\n",
    "* generic method for creating algorithn inputs\n",
    "* generic method for compiling model (including losses and metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pretrain models\n",
    "\n",
    "Use the following cell block to start building your pretraining model using the backbone network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fine-tune models\n",
    "\n",
    "Use the following cell block to start building your fine-tuning model using the backbone network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Use the following cell block to train your deep clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Based on the tutorial discussion, use the following cells to calculate model performance. The following metrics should be calculated:\n",
    "\n",
    "* Dice score coefficient (mean, median, 25th percentile, 75th percentile)\n",
    "\n",
    "### Performance\n",
    "\n",
    "The following minimum performance metrics must be met for full credit:\n",
    "\n",
    "* median Dice score coefficient: >0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "When ready, create a `*.csv` file with your compiled **validation** cohort sensitivity and Dice score statistics. There is no need to submit training performance accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Use the following line to save your model for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "training.save('./model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canvas\n",
    "\n",
    "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
    "\n",
    "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
    "* final (results) spreadsheet: `[UCInetID]_results.csv`\n",
    "* final (trained) model: `[UCInetID]_model.hdf5`\n",
    "\n",
    "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadshhet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
